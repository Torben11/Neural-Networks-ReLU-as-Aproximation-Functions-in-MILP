{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "[Bound Tightening](#BoundTightening)    \n",
    "  - [Prototype for bound tightening procedures](#Prototypeforboundtighteningprocedures)\n",
    "  - [RR](#RR)\n",
    "  - [ITER-RR](#ITER-RR) \n",
    "  - [LR](#LR)\n",
    "  \n",
    "[MILP Model](#MILPModel)  \n",
    "[Full Model](#Fullmodel)  \n",
    "[Full Model Hyperbolic](#FullModelHyperbolic)  \n",
    "[Neural Net](#NeuralNet)      \n",
    "[Previous Implementations](#PreviousImplementations) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gurobipy import *\n",
    "import pickle\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from neural_net_1 import neural_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bound Tightening <a name=\"BoundTightening\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_neurons(weights):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    weights : Weights of the neural network\n",
    "    \n",
    "    Output\n",
    "    t : total number of neurons in the neural network  \n",
    "    \"\"\"\n",
    "    t = 0\n",
    "    for layer in weights:\n",
    "        t += len(layer[0][0])\n",
    "    return t\n",
    "\n",
    "# função desatualizada\n",
    "def inicialize_bounds(weights):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    weights : Weights of the neural network\n",
    "    \n",
    "    Output\n",
    "    bounds : a zeros numpy array shapped as (total_neurons + len(input_dim), input_dim)\n",
    "    \"\"\"\n",
    "    t = total_neurons(weights) + len(weights[0])\n",
    "    bounds = np.zeros((t,2))\n",
    "    return bounds\n",
    "\n",
    "# função desatualizada\n",
    "def bound_model(model, x, s, k, j, maximize):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    model : model to optimize\n",
    "    x, s  : variables of the model\n",
    "    k, j  : layer and neuron, respectively\n",
    "    maximize : objetive of the model (True == maximize False == minimize)\n",
    "    \n",
    "    Output\n",
    "    new_bound : bound generated after the bound tightening procedure\n",
    "    \"\"\"\n",
    "    if maximize:\n",
    "        model.setObjective(x[k,j] - s[k,j], GRB.MAXIMIZE)\n",
    "        model.optimize()\n",
    "        new_bound = model.ObjVal\n",
    "    else:\n",
    "        model.setObjective(x[k,j] - s[k,j], GRB.MINIMIZE)\n",
    "        model.optimize()\n",
    "        new_bound = model.ObjVal\n",
    "    return new_bound\n",
    "\n",
    "\n",
    "def init_bounds(weights, init):\n",
    "    \"\"\"\n",
    "    weights : weights of the neural network\n",
    "    init : initial value of the bounds (lb = -init, ub = init)\n",
    "    \"\"\"   \n",
    "    # first outer list : layers\n",
    "    # second outer list : neurons\n",
    "    # third outer list : lower and upper bound, respectively\n",
    "    bounds = [[[-init,init] for j in range(len(weights[k][0]))] for k in range(len(weights))]\n",
    "    bounds.append([[-init,init]])\n",
    "    return bounds\n",
    "\n",
    "\n",
    "def show_bounds(bounds):\n",
    "    for i, layer in enumerate(bounds):\n",
    "        if i == 0:\n",
    "            print(\"Input Layer\")\n",
    "        elif i == len(bounds) - 1:\n",
    "            print(\"Output Layer\")\n",
    "        else:\n",
    "            print(\"Layer : {}\".format(i+1))\n",
    "        for neuron in layer:\n",
    "            print(\"Lower Bound : {} \\t Upper Bound : {}\".format(neuron[0],neuron[1]))\n",
    "            \n",
    "def bound_objective(model, x, s, k, j, maximize):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    model : model to optimize\n",
    "    x, s  : variables of the model\n",
    "    k, j  : layer and neuron, respectively\n",
    "    maximize : objetive of the model (True == maximize False == minimize)\n",
    "    \n",
    "    Output\n",
    "    new_bound : bound generated after the bound tightening procedure\n",
    "    \"\"\"\n",
    "    if maximize:\n",
    "        model.setObjective(x[k][j] - s[k,j], GRB.MAXIMIZE)\n",
    "        model.optimize()\n",
    "        new_bound = model.ObjVal\n",
    "    else:\n",
    "        model.setObjective(x[k][j] - s[k,j], GRB.MINIMIZE)\n",
    "        model.optimize()\n",
    "        new_bound = model.ObjVal\n",
    "    return new_bound\n",
    "\n",
    "\n",
    "def neurons_at(W, layer):\n",
    "    # Return the number of neurons of each layer\n",
    "    return len(W[layer][0][0])\n",
    "\n",
    "\n",
    "def empty_model(weights, parameters, bounds):\n",
    "    \n",
    "    # Create the model\n",
    "    m = Model('RR')\n",
    "    m.Params.LogToConsole = 0 # turn off logs\n",
    "       \n",
    "    # Local Variables\n",
    "    K = len(weights) # Number of layers in the neural network\n",
    "    nK = max(map(lambda l: len(l[0][0]), W))  # Size of the biggest layer\n",
    "      \n",
    "    # Model Variables\n",
    "    x = [[m.addVar(lb=neuron[0], ub=neuron[1]) for neuron in layer] for layer in bounds[:K]] # decision variables\n",
    "    s = m.addVars(K, nK, lb=0, ub=10) # negative dump\n",
    "    z = m.addVars(K, nK, lb=0, ub=1) # binary activation variable\n",
    "    y = m.addVar(lb=bounds[K][0][0],ub=bounds[K][0][1]) # output variable\n",
    "   \n",
    "    \n",
    "    #\"\"\"\n",
    "    # Input parameters\n",
    "    mean_x = parameters['mean_x']\n",
    "    std_x = parameters['std_x']\n",
    "    mean_y = parameters['mean_y']\n",
    "    std_y = parameters['std_y']\n",
    "    \n",
    "    # Constraints of the upper and lower bounds of normalized x0\n",
    "    m.addConstr( x[0][0] >= (-1 - mean_x[0])/std_x[0] ) # \"-1\" and \"1\" are the maximum and minimum values of x0, respectively\n",
    "    m.addConstr( x[0][0] <= (1 - mean_x[0])/std_x[0] )   \n",
    "    \n",
    "    # Constraints of upper and lower bounds of normalized x1\n",
    "    m.addConstr( x[0][1] >= (-1 - mean_x[1])/std_x[1] )\n",
    "    m.addConstr( x[0][1] <= (1 - mean_x[1])/std_x[1] )\n",
    "    #\"\"\"\n",
    "    \n",
    "    \n",
    "    for k in range(1, K+1): # Iterate over the hidden layers unitl it reaches the output layer of the neural net\n",
    "        neuronios = neurons_at(weights, k-1) # Number of neurons of each layer\n",
    "        for j in range(neuronios): # Iterate over the neurons of the layer\n",
    "            if k != K: # HIDDEN LAYERS\n",
    "                \"\"\"\n",
    "                Weights : W[layer][weights = 0 or bias = 1][inputs of the previous layer][neuron]\n",
    "                Input   : x[layer][neuron]\n",
    "                Negative part : s[layer][variable]               \n",
    "                \"\"\" \n",
    "                \n",
    "                # Rule of the hidden layers output ( W^{k-1} * x^{k-1} + BIAS = x^k - s^k ) \n",
    "                m.addConstr( sum( weights[k-1][0][i][j] * x[k-1][i] for i in range(len(weights[k-1][0])))\n",
    "                    + weights[k-1][1][j] == x[k][j] - s[k,j] )\n",
    "                \n",
    "                \n",
    "                # Classic ReLu Formulation\n",
    "                m.addConstr( x[k][j] <= bounds[k][j][1] * z[k,j] )\n",
    "                m.addConstr( s[k,j] <= -bounds[k][j][0] * (1 - z[k,j]) )\n",
    "                m.addConstr( x[k][j] >= 0)\n",
    "                \n",
    "                \n",
    "                if bounds[k][j][0] > 0:\n",
    "                    z[k,j].setAttr(GRB.Attr.LB, 1)  # Neuron activated\n",
    "                if bounds[k][j][1] < 0:\n",
    "                    z[k,j].setAttr(GRB.Attr.UB, 0)  # Neuron deactivated\n",
    "                               \n",
    "            else: # OUTPUT LAYER\n",
    "                # Rule of the output layer ( W^{k-1} * x^{k-1} + BIAS = y )\n",
    "                \n",
    "                m.addConstr( sum( weights[k-1][0][i][j] * x[k-1][i] for i in range(len(weights[k-1][0]))) + weights[k-1][1][j] == y )\n",
    "            \n",
    "                  \n",
    "    m.update()\n",
    "    \n",
    "    return m, x, s, z, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototypes for bound tightening procedures <a name=\"Prototypeforboundtighteningprocedures\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prototype_0(parameters, weights, bounds):\n",
    "    \"\"\"\n",
    "    Prototype 0 : Where we create the constraints right before calculating the bounds\n",
    "    Each bound is calculated based on the constraints of the layers 0 to k\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the model\n",
    "    m = Model('bound_tightening')\n",
    "    m.Params.LogToConsole = 0 # turn off logs\n",
    "    \n",
    "    # Local Variables\n",
    "    K = len(weights) # Number of layers in the neural network\n",
    "    nK = max(map(lambda l: len(l[0][0]), W))  # Size of the biggest layer\n",
    "    \n",
    "    # Model Variables\n",
    "    x = [[m.addVar(lb=neuron[0], ub=neuron[1]) for neuron in layer] for layer in bounds[:K]] # decision variables\n",
    "    s = m.addVars(K, nK, lb=0, ub=10) # negative dump\n",
    "    z = m.addVars(K, nK, lb=0, ub=1) # binary activation variable\n",
    "    y = m.addVar(lb=-5,ub=5) # output variable\n",
    "       \n",
    "    # Input parameters\n",
    "    mean_x = parameters['mean_x']\n",
    "    std_x = parameters['std_x']\n",
    "    mean_y = parameters['mean_y']\n",
    "    std_y = parameters['std_y']\n",
    "    \n",
    "    # Constraints of the upper and lower bounds of normalized x0\n",
    "    m.addConstr( x[0][0] >= (-1 - mean_x[0])/std_x[0] ) # \"-1\" and \"1\" are the maximum and minimum values of x0, respectively\n",
    "    m.addConstr( x[0][0] <= (1 - mean_x[0])/std_x[0] )   \n",
    "    \n",
    "    # Constraints of upper and lower bounds of normalized x1\n",
    "    m.addConstr( x[0][1] >= (-1 - mean_x[1])/std_x[1] )\n",
    "    m.addConstr( x[0][1] <= (1 - mean_x[1])/std_x[1] )\n",
    "    \n",
    "    for i in range(len(W[0])): # iterates over the neural net inputs\n",
    "        # Bounds of input\n",
    "        m.setObjective(x[0][i], GRB.MAXIMIZE)\n",
    "        m.optimize()\n",
    "        ub = m.ObjVal\n",
    "        m.setObjective(x[0][i], GRB.MINIMIZE)\n",
    "        m.optimize()\n",
    "        lb = m.ObjVal\n",
    "        bounds[0][i][0], bounds[0][i][1] = lb, ub\n",
    "    \n",
    "    for k in range(1, K+1): # Iterate over the hidden layers unitl it reaches the output layer of the neural net\n",
    "        neuronios = neurons_at(weights, k-1) # Number of neurons of each layer\n",
    "        for j in range(neuronios): # Iterate over the neurons of the layer\n",
    "            if k != K: # HIDDEN LAYERS\n",
    "                \"\"\"\n",
    "                Weights : W[layer][weights = 0 or bias = 1][inputs of the previous layer][neuron]\n",
    "                Input   : x[layer][variable]\n",
    "                Negative part : s[layer][variable]\n",
    "                \"\"\" \n",
    "                \n",
    "                # Rule of the hidden layers output ( W^{k-1} * x^{k-1} + BIAS = x^k - s^k ) \n",
    "                m.addConstr( sum( weights[k-1][0][i][j] * x[k-1][i] for i in range(len(weights[k-1][0])))\n",
    "                    + weights[k-1][1][j] == x[k][j] - s[k,j] )\n",
    "                \n",
    "                bounds[k][j][1] = bound_objective(m, x, s, k, j, True) # upper bound \n",
    "                bounds[k][j][0] = bound_objective(m, x, s, k, j, False) # lower bound\n",
    "                \n",
    "                # Classic ReLu Formulation\n",
    "                m.addConstr( x[k][j] <= bounds[k][j][1] * z[k,j] )\n",
    "                m.addConstr( s[k,j] <= -bounds[k][j][0] * (1 - z[k,j]) )\n",
    "                m.addConstr( x[k][j] >= 0)\n",
    "                               \n",
    "            else: # OUTPUT LAYER\n",
    "                # Rule of the output layer ( W^{k-1} * x^{k-1} + BIAS = y )\n",
    "                m.addConstr( sum( weights[k-1][0][i][j] * x[k-1][i] for i in range(len(weights[k-1][0]))) + weights[k-1][1][j] == y )\n",
    "                \n",
    "                # Bounds of output\n",
    "                m.setObjective(y, GRB.MAXIMIZE)\n",
    "                m.optimize()\n",
    "                ub = m.ObjVal\n",
    "                m.setObjective(y, GRB.MINIMIZE)\n",
    "                m.optimize()\n",
    "                lb = m.ObjVal\n",
    "                bounds[k][j][0], bounds[k][j][1] = lb, ub\n",
    "                \n",
    "    return bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------\n",
      "Warning: your license will expire in 3 days\n",
      "--------------------------------------------\n",
      "\n",
      "Using license file C:\\Users\\torbe\\gurobi.lic\n",
      "Academic license - for non-commercial use only\n",
      "Input Layer\n",
      "Lower Bound : -1.7277260797025042 \t Upper Bound : 1.7277260797025047\n",
      "Lower Bound : -1.7277260797023208 \t Upper Bound : 1.727726079702321\n",
      "Layer : 2\n",
      "Lower Bound : -0.27340875284809796 \t Upper Bound : 0.6420543910755608\n",
      "Lower Bound : -0.7328680719595796 \t Upper Bound : 0.9671934808951266\n",
      "Lower Bound : -0.488341184167866 \t Upper Bound : 1.1737879233646435\n",
      "Lower Bound : -1.1525504335676777 \t Upper Bound : 0.23313740342012518\n",
      "Lower Bound : -0.2540293587398855 \t Upper Bound : 1.1509430063915578\n",
      "Lower Bound : -0.20633246035525063 \t Upper Bound : 1.0316915068144488\n",
      "Lower Bound : -0.7538024692852211 \t Upper Bound : 0.8016267440337848\n",
      "Lower Bound : -0.8386853599404868 \t Upper Bound : 0.5066104197358665\n",
      "Lower Bound : -0.11309143250684223 \t Upper Bound : 1.0571106905291505\n",
      "Lower Bound : -0.715812832095168 \t Upper Bound : 1.0456498265047292\n",
      "Layer : 3\n",
      "Lower Bound : -1.18742514287407 \t Upper Bound : -0.229431334700232\n",
      "Lower Bound : -0.2468117890894993 \t Upper Bound : 0.8295005909964841\n",
      "Lower Bound : -0.2606524246514019 \t Upper Bound : 0.5584420963486674\n",
      "Lower Bound : -0.49654191669672443 \t Upper Bound : 0.8984834301543305\n",
      "Lower Bound : -0.16462877044121615 \t Upper Bound : 0.9607304714572589\n",
      "Lower Bound : -0.3438091303631592 \t Upper Bound : 1.2199354842234404\n",
      "Lower Bound : -0.07704481243884143 \t Upper Bound : 1.1224763334993249\n",
      "Lower Bound : -0.7081942767750746 \t Upper Bound : 0.2738205146625514\n",
      "Lower Bound : 0.05015983467193319 \t Upper Bound : 0.7502885168600901\n",
      "Lower Bound : -0.18671328041904367 \t Upper Bound : 0.7297584227088298\n",
      "Layer : 4\n",
      "Lower Bound : -0.684221080622109 \t Upper Bound : 0.6083736893189364\n",
      "Lower Bound : -0.44320637004137176 \t Upper Bound : 1.0080635764105423\n",
      "Lower Bound : -0.13275512820726032 \t Upper Bound : 1.5012146961815063\n",
      "Lower Bound : -0.18326088680492347 \t Upper Bound : 1.1548841541684176\n",
      "Lower Bound : -0.6755820837385018 \t Upper Bound : 1.0356056912116673\n",
      "Lower Bound : -2.899117242498929 \t Upper Bound : 0.20166310340505866\n",
      "Lower Bound : -0.5654415422165773 \t Upper Bound : 1.2006704139303057\n",
      "Lower Bound : -0.8851226238863958 \t Upper Bound : 0.32391550043933215\n",
      "Lower Bound : -0.9954301984889034 \t Upper Bound : -0.15487345517631823\n",
      "Lower Bound : -0.13892282163045788 \t Upper Bound : 1.4018155510002934\n",
      "Output Layer\n",
      "Lower Bound : -2.2465518039490213 \t Upper Bound : 1.7364051413861032\n"
     ]
    }
   ],
   "source": [
    "W = pickle.load(open('model.pickle', 'rb'))\n",
    "parameters = neural_init()\n",
    "initial_bounds = init_bounds(W, 5) # initialize the bounds\n",
    "bounds = prototype_0(parameters, W, initial_bounds)\n",
    "show_bounds(bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prototype_1(parameters, weights):\n",
    "    \"\"\"\n",
    "    Prototype 1 : Where we create all the constraints and variables before we calculate the bounds\n",
    "    Each bound is calculated based on the constraints of the layers 0 to K+1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the model\n",
    "    bounds = init_bounds(weights, 5) # initialize the bounds\n",
    "    m, x, s, z, y = empty_model(weights, parameters, bounds)\n",
    "    \n",
    "    K = len(weights) # Number of layers in the neural network\n",
    "    \n",
    "    \n",
    "        \n",
    "    for i in range(len(W[0])): # iterates over the neural net inputs\n",
    "        # Bounds of input\n",
    "        m.setObjective(x[0][i], GRB.MAXIMIZE)\n",
    "        m.optimize()\n",
    "        ub = m.ObjVal\n",
    "        m.setObjective(x[0][i], GRB.MINIMIZE)\n",
    "        m.optimize()\n",
    "        lb = m.ObjVal\n",
    "        bounds[0][i][0], bounds[0][i][1] = lb, ub\n",
    "    \n",
    "    for k in range(1, K+1): # Iterate over the hidden layers unitl it reaches the output layer of the neural net\n",
    "        neuronios = neurons_at(weights, k-1) # Number of neurons of each layer\n",
    "        for j in range(neuronios): # Iterate over the neurons of the layer\n",
    "            if k != K: # HIDDEN LAYERS\n",
    "                \"\"\"\n",
    "                Weights : W[layer][weights = 0 or bias = 1][inputs of the previous layer][neuron]\n",
    "                Input   : x[layer][variable]\n",
    "                Negative part : s[layer][variable]              \n",
    "                \"\"\" \n",
    "                \n",
    "                \n",
    "                bounds[k][j][1] = bound_objective(m, x, s, k, j, True) # upper bound \n",
    "                bounds[k][j][0] = bound_objective(m, x, s, k, j, False) # lower bound\n",
    "                               \n",
    "            else: # OUTPUT LAYER\n",
    "                \n",
    "                # Bounds of output\n",
    "                m.setObjective(y, GRB.MAXIMIZE)\n",
    "                m.optimize()\n",
    "                ub = m.ObjVal\n",
    "                m.setObjective(y, GRB.MINIMIZE)\n",
    "                m.optimize()\n",
    "                lb = m.ObjVal\n",
    "                bounds[k][j][0], bounds[k][j][1] = lb, ub\n",
    "                \n",
    "    return bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Layer\n",
      "Lower Bound : -1.7277260797025042 \t Upper Bound : 1.7277260797025047\n",
      "Lower Bound : -1.7277260797023208 \t Upper Bound : 1.7277260797023208\n",
      "Layer : 2\n",
      "Lower Bound : -0.27340875284809796 \t Upper Bound : 0.6420543910755612\n",
      "Lower Bound : -0.7328680719595797 \t Upper Bound : 0.9671934808951266\n",
      "Lower Bound : -0.488341184167866 \t Upper Bound : 1.1737879233646433\n",
      "Lower Bound : -1.1525504335676777 \t Upper Bound : 0.23313740342012518\n",
      "Lower Bound : -0.2540293587398854 \t Upper Bound : 1.1509430063915578\n",
      "Lower Bound : -0.20633246035525063 \t Upper Bound : 1.0316915068144488\n",
      "Lower Bound : -0.7538024692852211 \t Upper Bound : 0.8016267440337848\n",
      "Lower Bound : -0.8386853599404867 \t Upper Bound : 0.5066104197358663\n",
      "Lower Bound : -0.11309143250684223 \t Upper Bound : 1.0571106905291505\n",
      "Lower Bound : -0.715812832095168 \t Upper Bound : 1.0456498265047292\n",
      "Layer : 3\n",
      "Lower Bound : -5.0 \t Upper Bound : 0.6668549724835865\n",
      "Lower Bound : -2.929534043051771 \t Upper Bound : 5.0\n",
      "Lower Bound : -3.405033483388069 \t Upper Bound : 2.4790261938580054\n",
      "Lower Bound : -2.527767585428295 \t Upper Bound : 2.399885077809453\n",
      "Lower Bound : -2.204308326487798 \t Upper Bound : 2.4742961742251466\n",
      "Lower Bound : -1.9254485356234392 \t Upper Bound : 5.0\n",
      "Lower Bound : -0.8081177830445325 \t Upper Bound : 5.0\n",
      "Lower Bound : -4.784822353619168 \t Upper Bound : 1.203079822027414\n",
      "Lower Bound : -4.277701117910793 \t Upper Bound : 2.852733954060937\n",
      "Lower Bound : -3.2353785918614624 \t Upper Bound : 2.696082817269307\n",
      "Layer : 4\n",
      "Lower Bound : -5.0 \t Upper Bound : 3.555549555026615\n",
      "Lower Bound : -3.213268713966249 \t Upper Bound : 4.513528016584244\n",
      "Lower Bound : -2.957012921268281 \t Upper Bound : 5.0\n",
      "Lower Bound : -2.1680184448273785 \t Upper Bound : 5.0\n",
      "Lower Bound : -4.85430422260648 \t Upper Bound : 5.0\n",
      "Lower Bound : -5.0 \t Upper Bound : 1.9284394020602085\n",
      "Lower Bound : -5.0 \t Upper Bound : 5.0\n",
      "Lower Bound : -3.675178801951277 \t Upper Bound : 2.980653614073575\n",
      "Lower Bound : -5.0 \t Upper Bound : 2.8562426799904133\n",
      "Lower Bound : -2.2831182000425247 \t Upper Bound : 5.0\n",
      "Output Layer\n",
      "Lower Bound : -5.0 \t Upper Bound : 5.0\n"
     ]
    }
   ],
   "source": [
    "W = pickle.load(open('model.pickle', 'rb'))\n",
    "parameters = neural_init()\n",
    "bounds = prototype_1(parameters, W)\n",
    "show_bounds(bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RR <a name=\"RR\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_model_RR(weights, parameters, bounds):\n",
    "    \n",
    "    # Create the model\n",
    "    m = Model('RR')\n",
    "    m.Params.LogToConsole = 0 # turn off logs\n",
    "       \n",
    "    # Local Variables\n",
    "    K = len(weights) # Number of layers in the neural network\n",
    "    nK = max(map(lambda l: len(l[0][0]), W))  # Size of the biggest layer\n",
    "      \n",
    "    # Model Variables\n",
    "    x = [[m.addVar(lb=neuron[0], ub=neuron[1]) for neuron in layer] for layer in bounds[:K]] # decision variables   \n",
    "    s = m.addVars(K, nK, lb=0, ub=10) # negative dump\n",
    "    z = m.addVars(K, nK, lb=0, ub=1) # binary activation variable\n",
    "    y = m.addVar(lb=bounds[K][0][0],ub=bounds[K][0][1]) # output variable\n",
    "    \n",
    " \n",
    "    m.update()\n",
    "    for k,layer in enumerate(x):\n",
    "        for j,neuron in enumerate(layer):\n",
    "            if neuron.ub < 0:         \n",
    "                neuron.ub = 0\n",
    "    m.update()\n",
    "    \n",
    "    #\"\"\"\n",
    "    # Input parameters\n",
    "    mean_x = parameters['mean_x']\n",
    "    std_x = parameters['std_x']\n",
    "    mean_y = parameters['mean_y']\n",
    "    std_y = parameters['std_y']\n",
    "    \n",
    "    # Constraints of the upper and lower bounds of normalized x0\n",
    "    m.addConstr( x[0][0] >= (-1 - mean_x[0])/std_x[0] ) # \"-1\" and \"1\" are the maximum and minimum values of x0, respectively\n",
    "    m.addConstr( x[0][0] <= (1 - mean_x[0])/std_x[0] )   \n",
    "    \n",
    "    # Constraints of upper and lower bounds of normalized x1\n",
    "    m.addConstr( x[0][1] >= (-1 - mean_x[1])/std_x[1] )\n",
    "    m.addConstr( x[0][1] <= (1 - mean_x[1])/std_x[1] )\n",
    "    #\"\"\"\n",
    "\n",
    "    \n",
    "    for k in range(1, K+1): # Iterate over the hidden layers unitl it reaches the output layer of the neural net\n",
    "        neuronios = neurons_at(weights, k-1) # Number of neurons of each layer\n",
    "        for j in range(neuronios): # Iterate over the neurons of the layer\n",
    "            if k != K: # HIDDEN LAYERS\n",
    "                \"\"\"\n",
    "                Weights : W[layer][weights = 0 or bias = 1][inputs of the previous layer][neuron]\n",
    "                Input   : x[layer][neuron]\n",
    "                Negative part : s[layer][variable]               \n",
    "                \"\"\" \n",
    "                \n",
    "                # Rule of the hidden layers output ( W^{k-1} * x^{k-1} + BIAS = x^k - s^k ) \n",
    "                m.addConstr( sum( weights[k-1][0][i][j] * x[k-1][i] for i in range(len(weights[k-1][0])))\n",
    "                    + weights[k-1][1][j] == x[k][j] - s[k,j] )\n",
    "                \n",
    "                # Classic ReLu Formulation\n",
    "                m.addConstr( x[k][j] <= bounds[k][j][1] * z[k,j], 'Uk{}j{}'.format(k,j))\n",
    "                m.addConstr( s[k,j] <= -bounds[k][j][0] * (1 - z[k,j]), 'Lk{}j{}'.format(k,j))\n",
    "                m.addConstr( x[k][j] >= 0)\n",
    "                \n",
    "                \"\"\"\n",
    "                if bounds[k][j][1] < 0:\n",
    "                    z[k,j].lb , z[k,j].ub = 0, 0\n",
    "                    m.update()\n",
    "                    # print(\"ub < 0, z = \",z[k,j].obj)\n",
    "                if bounds[k][j][0] > 0:\n",
    "                    z[k,j].lb , z[k,j].ub = 1, 1\n",
    "                    m.update()\n",
    "                    # print(\"lb > 0, z = \",z[k,j].obj)\n",
    "                #\"\"\"\n",
    "                                             \n",
    "            else: # OUTPUT LAYER\n",
    "                # Rule of the output layer ( W^{k-1} * x^{k-1} + BIAS = y )\n",
    "                \n",
    "                m.addConstr( sum( weights[k-1][0][i][j] * x[k-1][i] for i in range(len(weights[k-1][0]))) + weights[k-1][1][j] == y )\n",
    "            \n",
    "                  \n",
    "    m.update()\n",
    "    \n",
    "    return m, x, s, z, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RR(parameters, weights, init_bounds):\n",
    "    \"\"\"\n",
    "    parameters : dictionary with some parameters of the neural net input \n",
    "    weights : numpy array with the neural net weights  \n",
    "    bounds : numpy array with the initial bounds\n",
    "    \"\"\"\n",
    "    \n",
    "    K = len(weights) # Number of layers in the neural network\n",
    "    bounds = deepcopy(init_bounds)\n",
    "    m, x, s, z, y = empty_model_RR(weights, parameters, bounds)\n",
    "     \n",
    "    #\"\"\"\n",
    "    for i in range(len(W[0])): # iterates over the neural net inputs\n",
    "        # Bounds of input\n",
    "        m.setObjective(x[0][i], GRB.MAXIMIZE)\n",
    "        m.optimize()\n",
    "        ub = m.ObjVal\n",
    "        m.setObjective(x[0][i], GRB.MINIMIZE)\n",
    "        m.optimize()\n",
    "        lb = m.ObjVal\n",
    "        bounds[0][i][0], bounds[0][i][1] = lb, ub\n",
    "    #\"\"\"\n",
    "    \"\"\"\n",
    "    for i in range(len(weights[0])): # iterates over the neural net inputs\n",
    "        # Bounds of input\n",
    "        bounds[0][i][0], bounds[0][i][1] = bounds[K][0][0], bounds[K][0][1]\n",
    "    #\"\"\"\n",
    "    for k in range(1, K+1): # Iterate over the hidden layers unitl it reaches the output layer of the neural net\n",
    "        neuronios = neurons_at(weights, k-1) # Number of neurons of each layer\n",
    "        for j in range(neuronios): # Iterate over the neurons of the layer\n",
    "            # print(k,j)\n",
    "            if k != K: # HIDDEN LAYERS\n",
    "                \"\"\"\n",
    "                Weights : W[layer][weights = 0 or bias = 1][inputs of the previous layer][neuron]\n",
    "                Input   : x[layer][variable]\n",
    "                Negative part : s[layer][variable]               \n",
    "                \"\"\"                 \n",
    "                # print('k',k,'j',j)\n",
    "                bounds[k][j][1] = bound_objective(m, x, s, k, j, True)  # upper bound \n",
    "                bounds[k][j][0] = bound_objective(m, x, s, k, j, False) # lower bound\n",
    "                \n",
    "                # print('k ',k,'j ',j ,'LB ',bounds[k][j][0] ,'UB ', bounds[k][j][1], 'ABS ', abs(bounds[k][j][1] - bounds[k][j][0]))\n",
    "                \n",
    "                # First Method (ITER-RR doesn't work, but the bounds are ok)\n",
    "                m.remove(m.getConstrByName('Uk{}j{}'.format(k,j)))\n",
    "                m.remove(m.getConstrByName('Lk{}j{}'.format(k,j)))\n",
    "                m.update()\n",
    "                m.addConstr( x[k][j] <= bounds[k][j][1] * z[k,j], 'Uk{}j{}'.format(k,j))\n",
    "                m.addConstr( s[k,j] <= -bounds[k][j][0] * (1 - z[k,j]), 'Lk{}j{}'.format(k,j))\n",
    "                \n",
    "                \n",
    "                assert bounds[k][j][1] >= bounds[k][j][0]\n",
    "\n",
    "                \n",
    "                \n",
    "                # Second Method (Not Working)\n",
    "                # Change the constraint by updating variable z coeffcient (bounds)\n",
    "                # m.chgCoeff(m.getConstrByName('Uk{}j{}'.format(k,j)), z[k,j], bounds[k][j][1]) \n",
    "                # m.chgCoeff(m.getConstrByName('Lk{}j{}'.format(k,j)), z[k,j], -bounds[k][j][0])\n",
    "                \n",
    "                m.update()\n",
    "\n",
    "                \n",
    "                               \n",
    "            else: # OUTPUT LAYER           \n",
    "                # Bounds of output     \n",
    "                m.setObjective(y, GRB.MAXIMIZE)\n",
    "                m.optimize()\n",
    "                ub = m.ObjVal\n",
    "                m.setObjective(y, GRB.MINIMIZE)\n",
    "                m.optimize()\n",
    "                lb = m.ObjVal\n",
    "                bounds[k][j][0], bounds[k][j][1] = lb, ub\n",
    "       \n",
    "            \n",
    "            assert bounds[k][j][1] >= bounds[k][j][0]\n",
    "                \n",
    "\n",
    "    \n",
    "    return bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Layer\n",
      "Lower Bound : -1.7277260797025042 \t Upper Bound : 1.7277260797025047\n",
      "Lower Bound : -1.7277260797023208 \t Upper Bound : 1.7277260797023208\n",
      "Layer : 2\n",
      "Lower Bound : -0.27340875284809796 \t Upper Bound : 0.6420543910755612\n",
      "Lower Bound : -0.7328680719595795 \t Upper Bound : 0.9671934808951264\n",
      "Lower Bound : -0.488341184167866 \t Upper Bound : 1.173787923364643\n",
      "Lower Bound : -1.1525504335676777 \t Upper Bound : 0.2331374034201248\n",
      "Lower Bound : -0.2540293587398854 \t Upper Bound : 1.1509430063915578\n",
      "Lower Bound : -0.20633246035525055 \t Upper Bound : 1.0316915068144479\n",
      "Lower Bound : -0.7538024692852211 \t Upper Bound : 0.8016267440337845\n",
      "Lower Bound : -0.8386853599404868 \t Upper Bound : 0.5066104197358667\n",
      "Lower Bound : -0.11309143250684195 \t Upper Bound : 1.057110690529151\n",
      "Lower Bound : -0.715812832095168 \t Upper Bound : 1.0456498265047292\n",
      "Layer : 3\n",
      "Lower Bound : -1.1874251428740699 \t Upper Bound : -0.22943133470023191\n",
      "Lower Bound : -0.24681178908949924 \t Upper Bound : 0.8295005909964845\n",
      "Lower Bound : -0.26065242465140187 \t Upper Bound : 0.5584420963486671\n",
      "Lower Bound : -0.4965419166967245 \t Upper Bound : 0.8984834301543305\n",
      "Lower Bound : -0.16462877044121793 \t Upper Bound : 0.9607304714572589\n",
      "Lower Bound : -0.3438091303631591 \t Upper Bound : 1.219935484223441\n",
      "Lower Bound : -0.07704481243884095 \t Upper Bound : 1.1224763334993249\n",
      "Lower Bound : -0.7081942767750751 \t Upper Bound : 0.2738205146625514\n",
      "Lower Bound : 0.05015983467193358 \t Upper Bound : 0.7502885168600901\n",
      "Lower Bound : -0.1867132804190435 \t Upper Bound : 0.7297584227088296\n",
      "Layer : 4\n",
      "Lower Bound : -0.684221080622109 \t Upper Bound : 0.6083736893189351\n",
      "Lower Bound : -0.44320637004137264 \t Upper Bound : 1.0080635764105423\n",
      "Lower Bound : -0.13275512820725943 \t Upper Bound : 1.5012146961815067\n",
      "Lower Bound : -0.18326088680492347 \t Upper Bound : 1.1548841541684176\n",
      "Lower Bound : -0.675582083738502 \t Upper Bound : 1.0356056912116678\n",
      "Lower Bound : -2.8991172424989293 \t Upper Bound : 0.20166310340506\n",
      "Lower Bound : -0.565441542216576 \t Upper Bound : 1.2006704139303048\n",
      "Lower Bound : -0.8851226238863958 \t Upper Bound : 0.3239155004393335\n",
      "Lower Bound : -0.9954301984889047 \t Upper Bound : -0.15487345517631823\n",
      "Lower Bound : -0.13892282163045758 \t Upper Bound : 1.401815551000293\n",
      "Output Layer\n",
      "Lower Bound : -2.2465518039490204 \t Upper Bound : 1.7364051413861032\n"
     ]
    }
   ],
   "source": [
    "initial_b = init_bounds(W,5)\n",
    "b1 = RR(parameters, W, initial_b)\n",
    "show_bounds(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Layer\n",
      "Lower Bound : -1.7277260797025042 \t Upper Bound : 1.7277260797025047\n",
      "Lower Bound : -1.7277260797023208 \t Upper Bound : 1.7277260797023208\n",
      "Layer : 2\n",
      "Lower Bound : -0.2734087528480979 \t Upper Bound : 0.6420543910755612\n",
      "Lower Bound : -0.7328680719595795 \t Upper Bound : 0.9671934808951264\n",
      "Lower Bound : -0.488341184167866 \t Upper Bound : 1.173787923364643\n",
      "Lower Bound : -1.1525504335676768 \t Upper Bound : 0.2331374034201248\n",
      "Lower Bound : -0.25402935873988575 \t Upper Bound : 1.1509430063915578\n",
      "Lower Bound : -0.20633246035525066 \t Upper Bound : 1.0316915068144479\n",
      "Lower Bound : -0.753802469285221 \t Upper Bound : 0.8016267440337845\n",
      "Lower Bound : -0.8386853599404865 \t Upper Bound : 0.5066104197358664\n",
      "Lower Bound : -0.11309143250684232 \t Upper Bound : 1.0571106905291505\n",
      "Lower Bound : -0.715812832095168 \t Upper Bound : 1.0456498265047292\n",
      "Layer : 3\n",
      "Lower Bound : -1.1874251428740699 \t Upper Bound : -0.22943133470023205\n",
      "Lower Bound : -0.24681178908949924 \t Upper Bound : 0.8295005909964845\n",
      "Lower Bound : -0.26065242465140187 \t Upper Bound : 0.5584420963486671\n",
      "Lower Bound : -0.4965419166967242 \t Upper Bound : 0.8984834301543305\n",
      "Lower Bound : -0.16462877044121793 \t Upper Bound : 0.9607304714572589\n",
      "Lower Bound : -0.3438091303631591 \t Upper Bound : 1.219935484223441\n",
      "Lower Bound : -0.07704481243884095 \t Upper Bound : 1.1224763334993249\n",
      "Lower Bound : -0.7081942767750751 \t Upper Bound : 0.2738205146625514\n",
      "Lower Bound : 0.05015983467193358 \t Upper Bound : 0.7502885168600901\n",
      "Lower Bound : -0.1867132804190435 \t Upper Bound : 0.7297584227088296\n",
      "Layer : 4\n",
      "Lower Bound : -0.684221080622109 \t Upper Bound : 0.6083736893189351\n",
      "Lower Bound : -0.44320637004137264 \t Upper Bound : 1.0080635764105423\n",
      "Lower Bound : -0.13275512820725943 \t Upper Bound : 1.5012146961815067\n",
      "Lower Bound : -0.18326088680492347 \t Upper Bound : 1.1548841541684176\n",
      "Lower Bound : -0.675582083738502 \t Upper Bound : 1.0356056912116678\n",
      "Lower Bound : -2.8991172424989293 \t Upper Bound : 0.20166310340506\n",
      "Lower Bound : -0.565441542216576 \t Upper Bound : 1.2006704139303048\n",
      "Lower Bound : -0.8851226238863958 \t Upper Bound : 0.3239155004393335\n",
      "Lower Bound : -0.9954301984889051 \t Upper Bound : -0.15487345517631937\n",
      "Lower Bound : -0.13892282163045758 \t Upper Bound : 1.401815551000293\n",
      "Output Layer\n",
      "Lower Bound : -2.2465518039490204 \t Upper Bound : 1.7364051413861032\n"
     ]
    }
   ],
   "source": [
    "b1 = init_bounds(W,5)\n",
    "for i in range(15):\n",
    "    b1 = RR(parameters, W, b1)\n",
    "\n",
    "show_bounds(b1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RR_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_model_RR_1(weights, parameters, bounds):\n",
    "    \"\"\"The same function as empty_model() but with a commented part\n",
    "       Some problems are occuring with the ITER-RR, with this the model at least can be executed when init = 10\n",
    "       \"\"\"\n",
    "    \n",
    "    # Create the model\n",
    "    m = Model('RR')\n",
    "    m.Params.LogToConsole = 0 # turn off logs\n",
    "       \n",
    "    # Local Variables\n",
    "    K = len(weights) # Number of layers in the neural network\n",
    "    nK = max(map(lambda l: len(l[0][0]), W))  # Size of the biggest layer\n",
    "    \n",
    "    \n",
    "    # Model Variables\n",
    "    x = [[m.addVar(lb=neuron[0], ub=neuron[1]) for neuron in layer] for layer in bounds[:K]] # decision variables\n",
    "    #\"\"\"\n",
    "    m.update()\n",
    "    for layer in x:\n",
    "        for neuron in layer:\n",
    "            if neuron.ub < 0:\n",
    "                neuron.ub = 0\n",
    "\n",
    "    m.update()\n",
    "    #\"\"\"\n",
    "    \n",
    "    s = m.addVars(K, nK, lb=0, ub=10) # negative dump\n",
    "    z = m.addVars(K, nK, lb=0, ub=1) # binary activation variable\n",
    "    y = m.addVar(lb=bounds[K][0][0],ub=bounds[K][0][1]) # output variable\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # Input parameters\n",
    "    mean_x = parameters['mean_x']\n",
    "    std_x = parameters['std_x']\n",
    "    mean_y = parameters['mean_y']\n",
    "    std_y = parameters['std_y']\n",
    "    \n",
    "    # Constraints of the upper and lower bounds of normalized x0\n",
    "    m.addConstr( x[0][0] >= (-1 - mean_x[0])/std_x[0] ) # \"-1\" and \"1\" are the maximum and minimum values of x0, respectively\n",
    "    m.addConstr( x[0][0] <= (1 - mean_x[0])/std_x[0] )   \n",
    "    \n",
    "    # Constraints of upper and lower bounds of normalized x1\n",
    "    m.addConstr( x[0][1] >= (-1 - mean_x[1])/std_x[1] )\n",
    "    m.addConstr( x[0][1] <= (1 - mean_x[1])/std_x[1] )\n",
    "    #\"\"\"\n",
    "\n",
    "    \n",
    "    for k in range(1, K+1): # Iterate over the hidden layers unitl it reaches the output layer of the neural net\n",
    "        neuronios = neurons_at(weights, k-1) # Number of neurons of each layer\n",
    "        for j in range(neuronios): # Iterate over the neurons of the layer\n",
    "            if k != K: # HIDDEN LAYERS\n",
    "                \"\"\"\n",
    "                Weights : W[layer][weights = 0 or bias = 1][inputs of the previous layer][neuron]\n",
    "                Input   : x[layer][neuron]\n",
    "                Negative part : s[layer][variable]               \n",
    "                \"\"\" \n",
    "                \n",
    "                # Rule of the hidden layers output ( W^{k-1} * x^{k-1} + BIAS = x^k - s^k ) \n",
    "                m.addConstr( sum( weights[k-1][0][i][j] * x[k-1][i] for i in range(len(weights[k-1][0])))\n",
    "                    + weights[k-1][1][j] == x[k][j] - s[k,j] )\n",
    "                \n",
    "                # Classic ReLu Formulation\n",
    "                m.addConstr( x[k][j] <= bounds[k][j][1] * z[k,j], 'Uk{}j{}'.format(k,j))\n",
    "                m.addConstr( s[k,j] <= -bounds[k][j][0] * (1 - z[k,j]), 'Lk{}j{}'.format(k,j))\n",
    "                m.addConstr( x[k][j] >= 0)\n",
    "                           \n",
    "                assert bounds[k][j][1] >= bounds[k][j][0]\n",
    "                if bounds[k][j][1] < 0:\n",
    "                    z[k,j].lb , z[k,j].ub = 0, 0\n",
    "                    m.update()\n",
    "                    # print(\"ub < 0, z = \",z[k,j].obj)\n",
    "                if bounds[k][j][0] > 0:\n",
    "                    z[k,j].lb , z[k,j].ub = 1, 1\n",
    "                    m.update()\n",
    "                    # print(\"lb > 0, z = \",z[k,j].obj)\n",
    "                    \n",
    "                \n",
    "                \n",
    "                \n",
    "                               \n",
    "            else: # OUTPUT LAYER\n",
    "                # Rule of the output layer ( W^{k-1} * x^{k-1} + BIAS = y )\n",
    "                \n",
    "                m.addConstr( sum( weights[k-1][0][i][j] * x[k-1][i] for i in range(len(weights[k-1][0]))) + weights[k-1][1][j] == y )\n",
    "            \n",
    "                  \n",
    "    m.update()\n",
    "    \n",
    "    return m, x, s, z, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RR_1(parameters, weights, init_bounds):\n",
    "    \"\"\"\n",
    "    parameters : dictionary with some parameters of the neural net input \n",
    "    weights : numpy array with the neural net weights  \n",
    "    bounds : numpy array with the initial bounds\n",
    "    \"\"\"\n",
    "    \n",
    "    bounds = deepcopy(init_bounds)\n",
    "    K = len(weights) # Number of layers in the neural network\n",
    "    \"\"\"\n",
    "    m, x, s, z, y = empty_model_RR_1(weights, parameters, bounds)\n",
    "    for i in range(len(weights[0])): # iterates over the neural net inputs\n",
    "        # Bounds of input\n",
    "        bounds[0][i][0], bounds[0][i][1] = bounds[K][0][0], bounds[K][0][1]\n",
    "    #\"\"\"\n",
    "    \n",
    "    for k in range(1, K+1): # Iterate over the hidden layers unitl it reaches the output layer of the neural net\n",
    "        neuronios = neurons_at(weights, k-1) # Number of neurons of each layer\n",
    "        m, x, s, z, y = empty_model_RR_1(weights, parameters, bounds)\n",
    "        for j in range(neuronios): # Iterate over the neurons of the layer\n",
    "            \n",
    "            \n",
    "            if k > 0 and k != K: # HIDDEN LAYERS\n",
    "                \"\"\"\n",
    "                Weights : W[layer][weights = 0 or bias = 1][inputs of the previous layer][neuron]\n",
    "                Input   : x[layer][variable]\n",
    "                Negative part : s[layer][variable]               \n",
    "                \"\"\"                 \n",
    "                \n",
    "                bounds[k][j][1] = bound_objective(m, x, s, k, j, True)  # upper bound \n",
    "                bounds[k][j][0] = bound_objective(m, x, s, k, j, False) # lower bound\n",
    "                #print(bounds[k][j], 'k ',k,'j ',j)\n",
    "                              \n",
    "            else: # OUTPUT LAYER           \n",
    "                # Bounds of output\n",
    "                m.setObjective(y, GRB.MAXIMIZE)\n",
    "                m.optimize()\n",
    "                ub = m.ObjVal\n",
    "                m.setObjective(y, GRB.MINIMIZE)\n",
    "                m.optimize()\n",
    "                lb = m.ObjVal\n",
    "                bounds[k][j][0], bounds[k][j][1] = lb, ub\n",
    "       \n",
    "            \n",
    "            \n",
    "                \n",
    "\n",
    "    \n",
    "    return bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Layer\n",
      "Lower Bound : -5 \t Upper Bound : 5\n",
      "Lower Bound : -5 \t Upper Bound : 5\n",
      "Layer : 2\n",
      "Lower Bound : -1.1403419449925423 \t Upper Bound : 1.508987583220005\n",
      "Lower Bound : -2.342807039618492 \t Upper Bound : 2.577132448554039\n",
      "Lower Bound : -2.0623585572466254 \t Upper Bound : 2.7478052964434028\n",
      "Lower Bound : -2.4647810654714704 \t Upper Bound : 1.5453680353239179\n",
      "Lower Bound : -1.5845222196076065 \t Upper Bound : 2.481435867259279\n",
      "Lower Bound : -1.3787270858883853 \t Upper Bound : 2.2040861323475838\n",
      "Lower Bound : -2.2267762552946806 \t Upper Bound : 2.2746005300432444\n",
      "Lower Bound : -2.1126651726663113 \t Upper Bound : 1.780590232461691\n",
      "Lower Bound : -1.2212595425080508 \t Upper Bound : 2.165278800530359\n",
      "Lower Bound : -2.3838979490101337 \t Upper Bound : 2.713734943419695\n",
      "Layer : 3\n",
      "Lower Bound : -2.9563850910208354 \t Upper Bound : -0.03503497925396859\n",
      "Lower Bound : -1.2174696747461002 \t Upper Bound : 2.607642947022047\n",
      "Lower Bound : -1.3717206783428464 \t Upper Bound : 1.15699707097663\n",
      "Lower Bound : -1.5861113564054197 \t Upper Bound : 1.659486997365815\n",
      "Lower Bound : -1.0769655296814735 \t Upper Bound : 1.5922330411312031\n",
      "Lower Bound : -1.2127235296605627 \t Upper Bound : 3.0821011614021856\n",
      "Lower Bound : -0.304735415940975 \t Upper Bound : 3.338033086456262\n",
      "Lower Bound : -2.4508146244608033 \t Upper Bound : 0.4709305056697879\n",
      "Lower Bound : -1.3793855942724274 \t Upper Bound : 1.3445966615903897\n",
      "Lower Bound : -1.195726027329846 \t Upper Bound : 1.7398336256052265\n",
      "Layer : 4\n",
      "Lower Bound : -2.3725910130112564 \t Upper Bound : 1.3850580008947238\n",
      "Lower Bound : -1.5274448730326409 \t Upper Bound : 2.0169300115397792\n",
      "Lower Bound : -0.45040521614560797 \t Upper Bound : 3.858531903995582\n",
      "Lower Bound : -0.4949697826429192 \t Upper Bound : 3.3717234027856424\n",
      "Lower Bound : -1.9657297978820156 \t Upper Bound : 1.926253439021219\n",
      "Lower Bound : -5.0 \t Upper Bound : 0.6175824278930815\n",
      "Lower Bound : -2.640861398153991 \t Upper Bound : 2.1914475084123337\n",
      "Lower Bound : -1.9229465461869029 \t Upper Bound : 0.8214567584651569\n",
      "Lower Bound : -2.488895855887641 \t Upper Bound : 0.6382908624633918\n",
      "Lower Bound : -1.184959573925228 \t Upper Bound : 2.5701628962926946\n",
      "Output Layer\n",
      "Lower Bound : -4.864861663386114 \t Upper Bound : 3.507726639160321\n"
     ]
    }
   ],
   "source": [
    "initial_b = init_bounds(W,5)    \n",
    "b1 = RR_1(parameters, W, initial_b)\n",
    "show_bounds(b1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Layer\n",
      "Lower Bound : -5 \t Upper Bound : 5\n",
      "Lower Bound : -5 \t Upper Bound : 5\n",
      "Layer : 2\n",
      "Lower Bound : -0.27340875284809796 \t Upper Bound : 0.6420543910755608\n",
      "Lower Bound : -0.7328680719595796 \t Upper Bound : 0.9671934808951267\n",
      "Lower Bound : -0.488341184167866 \t Upper Bound : 1.1737879233646433\n",
      "Lower Bound : -1.1525504335676777 \t Upper Bound : 0.23313740342012518\n",
      "Lower Bound : -0.2540293587398855 \t Upper Bound : 1.1509430063915578\n",
      "Lower Bound : -0.20633246035525063 \t Upper Bound : 1.0316915068144488\n",
      "Lower Bound : -0.753802469285221 \t Upper Bound : 0.8016267440337848\n",
      "Lower Bound : -0.8386853599404868 \t Upper Bound : 0.5066104197358665\n",
      "Lower Bound : -0.11309143250684223 \t Upper Bound : 1.0571106905291505\n",
      "Lower Bound : -0.715812832095168 \t Upper Bound : 1.0456498265047292\n",
      "Layer : 3\n",
      "Lower Bound : -1.18742514287407 \t Upper Bound : -0.22943133470023205\n",
      "Lower Bound : -0.24681178908949963 \t Upper Bound : 0.8295005909964843\n",
      "Lower Bound : -0.26065242465140187 \t Upper Bound : 0.5584420963486671\n",
      "Lower Bound : -0.49654191669672426 \t Upper Bound : 0.8984834301543305\n",
      "Lower Bound : -0.1646287704412175 \t Upper Bound : 0.9607304714572587\n",
      "Lower Bound : -0.3438091303631592 \t Upper Bound : 1.2199354842234404\n",
      "Lower Bound : -0.077044812438841 \t Upper Bound : 1.1224763334993249\n",
      "Lower Bound : -0.708194276775075 \t Upper Bound : 0.27382051466254964\n",
      "Lower Bound : 0.05015983467193319 \t Upper Bound : 0.7502885168600899\n",
      "Lower Bound : -0.1867132804190437 \t Upper Bound : 0.7297584227088296\n",
      "Layer : 4\n",
      "Lower Bound : -0.6842210806221094 \t Upper Bound : 0.6083736893189347\n",
      "Lower Bound : -0.443206370041372 \t Upper Bound : 1.0080635764105423\n",
      "Lower Bound : -0.13275512820726 \t Upper Bound : 1.5012146961815067\n",
      "Lower Bound : -0.18326088680492275 \t Upper Bound : 1.1548841541684176\n",
      "Lower Bound : -0.675582083738502 \t Upper Bound : 1.035605691211667\n",
      "Lower Bound : -2.8991172424989298 \t Upper Bound : 0.20166310340505866\n",
      "Lower Bound : -0.565441542216576 \t Upper Bound : 1.2006704139303057\n",
      "Lower Bound : -0.8851226238863957 \t Upper Bound : 0.32391550043933226\n",
      "Lower Bound : -0.9954301984889048 \t Upper Bound : -0.15487345517631929\n",
      "Lower Bound : -0.1389228216304575 \t Upper Bound : 1.401815551000292\n",
      "Output Layer\n",
      "Lower Bound : -2.2465518039490204 \t Upper Bound : 1.7364051413861024\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    b1 = deepcopy(RR_1(parameters, W, b1))\n",
    "show_bounds(b1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ITER-RR <a name=\"ITER-RR\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAD(bounds):\n",
    "    soma = 0\n",
    "    i = 0\n",
    "    for layer in bounds:\n",
    "        for neuron in layer:\n",
    "            i += 1\n",
    "            soma += abs(neuron[1] - neuron[0])\n",
    "    return soma / i\n",
    "            \n",
    "    \n",
    "def ITER_RR(parameters, weights, init_bounds, max_iter, ref):\n",
    "    \n",
    "    i = 0 # iterator\n",
    "    ratio = 0 # [0-1] range\n",
    "    m_0 = MAD(init_bounds) # MAD of the initial bounds\n",
    "    bounds = deepcopy(init_bounds) \n",
    "    \n",
    "    while ratio <= ref  and  i <= max_iter:\n",
    "        bounds = RR_1(parameters, weights, bounds)\n",
    "        m = MAD(bounds)\n",
    "        ratio = m / m_0\n",
    "        m_0 = m\n",
    "        print(ratio)\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    return bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4255904360997351\n",
      "0.9900944918894189\n",
      "0.99922983270076\n",
      "Input Layer\n",
      "Lower Bound : -5 \t Upper Bound : 5\n",
      "Lower Bound : -5 \t Upper Bound : 5\n",
      "Layer : 2\n",
      "Lower Bound : -1.1052279687114324 \t Upper Bound : 1.508987583220005\n",
      "Lower Bound : -2.1322563431356754 \t Upper Bound : 2.5771324485540394\n",
      "Lower Bound : -2.0528247002910107 \t Upper Bound : 2.7478052964434028\n",
      "Lower Bound : -2.4647810654714704 \t Upper Bound : 1.5323305298034935\n",
      "Lower Bound : -1.5845222196076065 \t Upper Bound : 2.480647783621844\n",
      "Lower Bound : -1.3682238557478035 \t Upper Bound : 2.2040861323475838\n",
      "Lower Bound : -2.2267762552946806 \t Upper Bound : 2.159159927774918\n",
      "Lower Bound : -2.0062097878076934 \t Upper Bound : 1.780590232461691\n",
      "Lower Bound : -1.2212595425080508 \t Upper Bound : 2.1634402839422857\n",
      "Lower Bound : -2.2734694795126296 \t Upper Bound : 2.713734943419695\n",
      "Layer : 3\n",
      "Lower Bound : -2.9563850910208354 \t Upper Bound : -0.038557296121993656\n",
      "Lower Bound : -1.215587999546331 \t Upper Bound : 2.55612276453088\n",
      "Lower Bound : -1.3582187803649233 \t Upper Bound : 1.1369786735065086\n",
      "Lower Bound : -1.5827271638862637 \t Upper Bound : 1.648200989164334\n",
      "Lower Bound : -1.0518533538888852 \t Upper Bound : 1.5874638615293535\n",
      "Lower Bound : -1.2111956784955547 \t Upper Bound : 3.0775902696935775\n",
      "Lower Bound : -0.3005239242663278 \t Upper Bound : 3.2872797893622745\n",
      "Lower Bound : -2.3394822819590178 \t Upper Bound : 0.4671029674662234\n",
      "Lower Bound : -1.332623225624428 \t Upper Bound : 1.3376494816883118\n",
      "Lower Bound : -1.1654375949308402 \t Upper Bound : 1.6994242301544564\n",
      "Layer : 4\n",
      "Lower Bound : -2.3572537802146343 \t Upper Bound : 1.364627539317942\n",
      "Lower Bound : -1.5205581311965803 \t Upper Bound : 1.9910033273341388\n",
      "Lower Bound : -0.44456737993148787 \t Upper Bound : 3.8016289860443493\n",
      "Lower Bound : -0.4887387304312344 \t Upper Bound : 3.3295331318139123\n",
      "Lower Bound : -1.957721723867897 \t Upper Bound : 1.9118850432210068\n",
      "Lower Bound : -5.0 \t Upper Bound : 0.6138458935118497\n",
      "Lower Bound : -2.5946555099753272 \t Upper Bound : 2.167582075173908\n",
      "Lower Bound : -1.9136124055331272 \t Upper Bound : 0.8150838067742001\n",
      "Lower Bound : -2.462057851938824 \t Upper Bound : 0.6208352525494933\n",
      "Lower Bound : -1.1781319889385438 \t Upper Bound : 2.546219219650596\n",
      "Output Layer\n",
      "Lower Bound : -4.819835615426353 \t Upper Bound : 3.4704371882847127\n"
     ]
    }
   ],
   "source": [
    "W = pickle.load(open('model.pickle', 'rb'))\n",
    "parameters = neural_init()\n",
    "initial_b = init_bounds(W,5)\n",
    "iter_bounds = ITER_RR(parameters, W, initial_b, 200, 0.999)\n",
    "show_bounds(iter_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR <a name=\"LR\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teste2(model,x,s,maximize,new,neuronios):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    model : model to optimize\n",
    "    x, s  : variables of the model\n",
    "    k, j  : layer and neuron, respectively\n",
    "    maximize : objetive of the model (True == maximize False == minimize)\n",
    "    \n",
    "    Output\n",
    "    new_bound : bound generated after the bound tightening procedure\n",
    "    \"\"\"\n",
    "    if maximize:\n",
    "        model.setObjective(x[new+neuronios] - s[new+neuronios], GRB.MAXIMIZE)\n",
    "        model.optimize()\n",
    "        new_bound = model.ObjVal\n",
    "    else:\n",
    "        model.setObjective(x[new+neuronios] - s[new+neuronios], GRB.MINIMIZE)\n",
    "        model.optimize()\n",
    "        new_bound = model.ObjVal\n",
    "    return new_bound\n",
    "\n",
    "\n",
    "\n",
    "def layer_relax(bounds, weights, k, j, previous_x, parameters):\n",
    "    m = Model('LRR_neuron')\n",
    "    m.Params.LogToConsole = 0 # turn off logs\n",
    "    \n",
    "    if k == 1:       \n",
    "        # Input parameters\n",
    "        mean_x = parameters['mean_x']\n",
    "        std_x = parameters['std_x']\n",
    "        mean_y = parameters['mean_y']\n",
    "        std_y = parameters['std_y']\n",
    "        \n",
    "        # Model Variables\n",
    "        previous_x = [m.addVar(lb=-1, ub=5) for i in range(len(weights[0][0]))] # decision variables\n",
    "        \n",
    "        # Constraints of the upper and lower bounds of normalized x0\n",
    "        m.addConstr( previous_x[0] >= (-1 - mean_x[0])/std_x[0] ) # \"-1\" and \"1\" are the maximum and minimum values of x0, respectively\n",
    "        m.addConstr( previous_x[0] <= (1 - mean_x[0])/std_x[0] )   \n",
    "\n",
    "        # Constraints of upper and lower bounds of normalized x1\n",
    "        m.addConstr( previous_x[1] >= (-1 - mean_x[1])/std_x[1] )\n",
    "        m.addConstr( previous_x[1] <= (1 - mean_x[1])/std_x[1] )\n",
    "\n",
    "        \n",
    "    x_0 = [m.addVar(lb=-1,ub=5) for i in range(len(previous_x))]\n",
    "    s = m.addVar(lb=0,ub=10)\n",
    "    z = m.addVar(lb=0,ub=1)\n",
    "    x = m.addVar(lb=-1,ub=5)\n",
    "    \n",
    "    for var1,var2 in zip(x_0,previous_x):\n",
    "        print(var1,var2)\n",
    "        m.addConstr(var1.obj == var2.obj)\n",
    "\n",
    "    # Rule of the hidden layers output ( W^{k-1} * x^{k-1} + BIAS = x^k - s^k )\n",
    "    m.addConstr( sum( weights[k-1][0][i][j] * x_0[i] for i in range(len(weights[k-1][0])))\n",
    "        + weights[k-1][1][j] == x - s )\n",
    "    \n",
    "    m.setObjective(x - s, GRB.MAXIMIZE)\n",
    "    m.optimize()\n",
    "    ub = m.ObjVal\n",
    "    \n",
    "    m.setObjective(x - s, GRB.MINIMIZE)\n",
    "    m.optimize()\n",
    "    lb = m.ObjVal\n",
    "    \n",
    "    \n",
    "    # Classic ReLu Formulation\n",
    "    m.addConstr( x <= bounds[aux,0] * z )\n",
    "    m.addConstr( s <= -bounds[aux,1] * (1 - z ))\n",
    "    m.addConstr( x >= 0)\n",
    "            \n",
    "    return ub, lb, x\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def LRR(parameters, weights):\n",
    "    \n",
    "    # Create the model\n",
    "    m = Model('bound_tightening')\n",
    "    m.Params.LogToConsole = 0 # turn off logs\n",
    "    \n",
    "    # Local Variables\n",
    "    bounds = inicialize_bounds(weights)\n",
    "    aux = 2 # index of the bounds array\n",
    "    K = len(weights) # Number of layers in the neural network\n",
    "    nK = max(map(lambda l: len(l[0][0]), W))  # Size of the biggest layer\n",
    "           \n",
    "    # Input parameters\n",
    "    mean_x = parameters['mean_x']\n",
    "    std_x = parameters['std_x']\n",
    "    mean_y = parameters['mean_y']\n",
    "    std_y = parameters['std_y']\n",
    "    \n",
    "    # Model Variables\n",
    "    x_0 = [m.addVar(lb=-1, ub=5) for i in range(len(weights[0][0]))] # decision variables\n",
    "    y = m.addVar(lb=-5,ub=5) # output variable\n",
    "    \n",
    "    # Constraints of the upper and lower bounds of normalized x0\n",
    "    m.addConstr( x_0[0] >= (-1 - mean_x[0])/std_x[0] ) # \"-1\" and \"1\" are the maximum and minimum values of x0, respectively\n",
    "    m.addConstr( x_0[0] <= (1 - mean_x[0])/std_x[0] )   \n",
    "    \n",
    "    # Constraints of upper and lower bounds of normalized x1\n",
    "    m.addConstr( x_0[1] >= (-1 - mean_x[1])/std_x[1] )\n",
    "    m.addConstr( x_0[1] <= (1 - mean_x[1])/std_x[1] )\n",
    "    \n",
    "    for i in range(len(W[0])): # iterates over the neural net inputs\n",
    "        # Bounds of input\n",
    "        m.setObjective(x_0[i], GRB.MAXIMIZE)\n",
    "        m.optimize()\n",
    "        ub = m.ObjVal\n",
    "        m.setObjective(x_0[i], GRB.MINIMIZE)\n",
    "        m.optimize()\n",
    "        lb = m.ObjVal\n",
    "        bounds[i,1], bounds[i,0] = lb, ub\n",
    "    \n",
    "    previous = [x_0]\n",
    "    current = [[]]\n",
    "    for k in range(1, K+1): # Iterate over the hidden layers unitl it reaches the output layer of the neural net\n",
    "        neuronios = neurons_at(weights, k-1) # Number of neurons of each layer\n",
    "        for j in range(neuronios): # Iterate over the neurons of the layer\n",
    "            if k != K: # HIDDEN LAYERS\n",
    "                \"\"\"\n",
    "                Weights : W[layer][weights = 0 or bias = 1][inputs of the previous layer][neuron]\n",
    "                Input   : x[layer][variable]\n",
    "                Negative part : s[layer][variable]\n",
    "                \"\"\" \n",
    "\n",
    "                # print('layer ',k, ' neuron ',j)\n",
    "                x = m.addVar(lb=-1,ub=5)\n",
    "                s = m.addVar(lb=0,ub=10) \n",
    "                z = m.addVar(lb=0,ub=1)          \n",
    "\n",
    "                # Rule of the hidden layers output ( W^{k-1} * x^{k-1} + BIAS = x^k - s^k )\n",
    "                m.addConstr( sum( weights[k-1][0][i][j] * previous[0][i] for i in range(len(weights[k-1][0])))\n",
    "                    + weights[k-1][1][j] == x - s )\n",
    "          \n",
    "                bounds[aux,0] = teste3(m,x,s,True)\n",
    "                bounds[aux,1] = teste3(m,x,s,False)\n",
    "                assert bounds[aux,0] >= bounds[aux,1]                   \n",
    "                \n",
    "                # Classic ReLu Formulation\n",
    "                m.addConstr( x <= bounds[aux,0] * z )\n",
    "                m.addConstr( s <= -bounds[aux,1] * (1 - z ))\n",
    "                m.addConstr( x >= 0)\n",
    "                current[0].append(x)            \n",
    "                \n",
    "                # m.remove(s)\n",
    "                # m.remove(z)\n",
    "                # m.update()\n",
    "                \n",
    "            else: # OUTPUT LAYER\n",
    "                # Rule of the output layer ( W^{k-1} * x^{k-1} + BIAS = y )\n",
    "                m.addConstr( sum( weights[k-1][0][i][j] * previous[0][i] for i in range(len(weights[k-1][0]))) + weights[k-1][1][j] == y )\n",
    "                \n",
    "                # Bounds of output\n",
    "                m.setObjective(y, GRB.MAXIMIZE)\n",
    "                m.optimize()\n",
    "                ub = m.ObjVal\n",
    "                m.setObjective(y, GRB.MINIMIZE)\n",
    "                m.optimize()\n",
    "                lb = m.ObjVal\n",
    "                bounds[aux,1], bounds[aux,0] = lb, ub        \n",
    "            \n",
    "            aux += 1\n",
    "        # for var in previous[0]:\n",
    "        #    m.remove(var)\n",
    "        m.update()\n",
    "         \n",
    "        previous[0] = current[0]\n",
    "        current[0] = []\n",
    "        \n",
    "    \n",
    "    return bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LRR(parameters, weights):\n",
    "    \n",
    "    # Create the model\n",
    "    m = Model('bound_tightening')\n",
    "    m.Params.LogToConsole = 0 # turn off logs\n",
    "    \n",
    "    # Local Variables\n",
    "    bounds = inicialize_bounds(weights)\n",
    "    aux = 2 # index of the bounds array\n",
    "    K = len(weights) # Number of layers in the neural network\n",
    "    nK = max(map(lambda l: len(l[0][0]), W))  # Size of the biggest layer\n",
    "           \n",
    "    # Input parameters\n",
    "    mean_x = parameters['mean_x']\n",
    "    std_x = parameters['std_x']\n",
    "    mean_y = parameters['mean_y']\n",
    "    std_y = parameters['std_y']\n",
    "    \n",
    "    # Model Variables\n",
    "    x_0 = [m.addVar(lb=-1, ub=5) for i in range(len(weights[0][0]))] # decision variables\n",
    "    y = m.addVar(lb=-5,ub=5) # output variable\n",
    "    \n",
    "    # Constraints of the upper and lower bounds of normalized x0\n",
    "    m.addConstr( x_0[0] >= (-1 - mean_x[0])/std_x[0] ) # \"-1\" and \"1\" are the maximum and minimum values of x0, respectively\n",
    "    m.addConstr( x_0[0] <= (1 - mean_x[0])/std_x[0] )   \n",
    "    \n",
    "    # Constraints of upper and lower bounds of normalized x1\n",
    "    m.addConstr( x_0[1] >= (-1 - mean_x[1])/std_x[1] )\n",
    "    m.addConstr( x_0[1] <= (1 - mean_x[1])/std_x[1] )\n",
    "    \n",
    "    for i in range(len(W[0])): # iterates over the neural net inputs\n",
    "        # Bounds of input\n",
    "        m.setObjective(x_0[i], GRB.MAXIMIZE)\n",
    "        m.optimize()\n",
    "        ub = m.ObjVal\n",
    "        m.setObjective(x_0[i], GRB.MINIMIZE)\n",
    "        m.optimize()\n",
    "        lb = m.ObjVal\n",
    "        bounds[i,1], bounds[i,0] = lb, ub\n",
    "    \n",
    "    previous = [x_0]\n",
    "    current = [[]]\n",
    "    for k in range(1, K+1): # Iterate over the hidden layers unitl it reaches the output layer of the neural net\n",
    "        neuronios = neurons_at(weights, k-1) # Number of neurons of each layer\n",
    "        for j in range(neuronios): # Iterate over the neurons of the layer\n",
    "            if k != K: # HIDDEN LAYERS\n",
    "                \"\"\"\n",
    "                Weights : W[layer][weights = 0 or bias = 1][inputs of the previous layer][neuron]\n",
    "                Input   : x[layer][variable]\n",
    "                Negative part : s[layer][variable]\n",
    "                \"\"\" \n",
    "\n",
    "                # print('layer ',k, ' neuron ',j)\n",
    "                x = m.addVar(lb=-1,ub=5)\n",
    "                s = m.addVar(lb=0,ub=10) \n",
    "                z = m.addVar(lb=0,ub=1)          \n",
    "\n",
    "                # Rule of the hidden layers output ( W^{k-1} * x^{k-1} + BIAS = x^k - s^k )\n",
    "                m.addConstr( sum( weights[k-1][0][i][j] * previous[0][i] for i in range(len(weights[k-1][0])))\n",
    "                    + weights[k-1][1][j] == x - s )\n",
    "          \n",
    "                bounds[aux,0] = teste3(m,x,s,True)\n",
    "                bounds[aux,1] = teste3(m,x,s,False)\n",
    "                assert bounds[aux,0] >= bounds[aux,1]                   \n",
    "                \n",
    "                # Classic ReLu Formulation\n",
    "                m.addConstr( x <= bounds[aux,0] * z )\n",
    "                m.addConstr( s <= -bounds[aux,1] * (1 - z ))\n",
    "                m.addConstr( x >= 0)\n",
    "                current[0].append(x)            \n",
    "                \n",
    "                # m.remove(s)\n",
    "                # m.remove(z)\n",
    "                # m.update()\n",
    "                \n",
    "            else: # OUTPUT LAYER\n",
    "                # Rule of the output layer ( W^{k-1} * x^{k-1} + BIAS = y )\n",
    "                m.addConstr( sum( weights[k-1][0][i][j] * previous[0][i] for i in range(len(weights[k-1][0]))) + weights[k-1][1][j] == y )\n",
    "                \n",
    "                # Bounds of output\n",
    "                m.setObjective(y, GRB.MAXIMIZE)\n",
    "                m.optimize()\n",
    "                ub = m.ObjVal\n",
    "                m.setObjective(y, GRB.MINIMIZE)\n",
    "                m.optimize()\n",
    "                lb = m.ObjVal\n",
    "                bounds[aux,1], bounds[aux,0] = lb, ub        \n",
    "            \n",
    "            aux += 1\n",
    "        # for var in previous[0]:\n",
    "        #    m.remove(var)\n",
    "        m.update()\n",
    "         \n",
    "        previous[0] = current[0]\n",
    "        current[0] = []\n",
    "        \n",
    "    \n",
    "    return bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'teste3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-fc9aa26c6c1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneural_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbounds_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLRR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mbounds_0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-61-16a364980cd9>\u001b[0m in \u001b[0;36mLRR\u001b[1;34m(parameters, weights)\u001b[0m\n\u001b[0;32m     60\u001b[0m                     + weights[k-1][1][j] == x - s )\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m                 \u001b[0mbounds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mteste3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m                 \u001b[0mbounds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mteste3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[1;32massert\u001b[0m \u001b[0mbounds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mbounds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'teste3' is not defined"
     ]
    }
   ],
   "source": [
    "W = pickle.load(open('model.pickle', 'rb'))\n",
    "parameters = neural_init()\n",
    "bounds_0 = LRR(parameters, W)\n",
    "bounds_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MILP Model <a name=\"MILPModel\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(parameters, weights):\n",
    "\n",
    "    m = Model('milp')\n",
    "    m.Params.LogToConsole = 0 # turn off logs\n",
    "    \n",
    "    K = len(weights) # Number of layers in the neural network\n",
    "    nK = max(map(lambda l: len(l[0][0]), W))  # Size of the biggest layer\n",
    "\n",
    "    \n",
    "    x = m.addVars(K, nK,lb=-1,ub=5) # decision variables\n",
    "    s = m.addVars(K, nK, lb=0, ub=10) # negative dump\n",
    "    z = m.addVars(K, nK, vtype=GRB.BINARY) # binary activation variable\n",
    "    y = m.addVar(lb=-5,ub=5) # output variable\n",
    "    \n",
    "    # Initial bounds\n",
    "    U = 5\n",
    "    L = -5\n",
    "    \n",
    "    mean_x = parameters['mean_x']\n",
    "    std_x = parameters['std_x']\n",
    "    mean_y = parameters['mean_y']\n",
    "\n",
    "    # Constraints of upper and lower bounds of normalized x0\n",
    "    # \"-1\" and \"1\" are the maximum and minimum values of x0, respectively\n",
    "    m.addConstr( x[0,0] >= (-1 - mean_x[0])/std_x[0] ) \n",
    "    m.addConstr( x[0,0] <= (1 - mean_x[0])/std_x[0] )   \n",
    "    \n",
    "    # Constraints of upper and lower bounds of normalized x1\n",
    "    m.addConstr( x[0,1] >= (-1 - mean_x[1])/std_x[1] )\n",
    "    m.addConstr( x[0,1] <= (1 - mean_x[1])/std_x[1] )\n",
    "    \n",
    "    #m.addConstr( x[0,1] >= (-1 - avg[1])/std[1] )\n",
    "    #m.addConstr( x[0,1] <= (1 - avg[1])/std[1] )\n",
    "    #m.addConstr( x[0,0] == (9.03 - avg[0])/std[0] )\n",
    "    #m.addConstr( x[0,1] == (200000 - avg[1])/std[1] )\n",
    "\n",
    "    M = 10000\n",
    "    t = 0.0\n",
    "\n",
    "    for k in range(1, K+1): # Iterate over the hidden layers unitl it reaches the output layer of the neural net\n",
    "        neuronios = len(weights[k-1][0][0]) # Number of neurons of each layer\n",
    "        for j in range(neuronios): # Iterate over the neurons of the layer\n",
    "            if k != K: # HIDDEN LAYERS\n",
    "                \"\"\"\n",
    "                Weights : W[layer][weights = 0 or bias = 1][inputs of the previous layer][neuron]\n",
    "                Input   : x[layer][variable]\n",
    "                Negative part : s[layer][variable]\n",
    "                \"\"\" \n",
    "                \n",
    "                # Rule of the hidden layers output ( W^{k-1} * x^{k-1} + BIAS = x^k - s^k ) \n",
    "                m.addConstr( sum( weights[k-1][0][i][j] * x[k-1,i] for i in range(len(weights[k-1][0])))\n",
    "                    + weights[k-1][1][j] == x[k,j] - s[k,j] )\n",
    "\n",
    "                # Hyperbolic\n",
    "                # m.addConstr( x[k,j] == 0.5 * (s[k,j] + a[k,j]) )\n",
    "                # m.addConstr( a[k,j] * a[k,j] == s[k,j] * s[k,j] + t * t )\n",
    "\n",
    "                # Big-M comum\n",
    "                #m.addConstr( x[k,j] >= s[k,j] )\n",
    "                #m.addConstr( x[k,j] >= 0 )\n",
    "                #m.addConstr( x[k,j] <= 0 + M * (1 - z[k,j]) )\n",
    "                #m.addConstr( x[k,j] <= s[k,j] + M * (z[k,j]) )\n",
    "    \n",
    "                # Classic ReLu Formulation\n",
    "                m.addConstr( x[k,j] <= U * z[k,j] )\n",
    "                m.addConstr( s[k,j] <= -L * (1 - z[k,j]) )\n",
    "                m.addConstr( x[k,j] >= 0)\n",
    "            else: # OUTPUT LAYER\n",
    "                # Rule of the output layer ( W^{k-1} * x^{k-1} + BIAS = y )\n",
    "                m.addConstr( sum( weights[k-1][0][i][j] * x[k-1,i] for i in range(len(weights[k-1][0]))) + weights[k-1][1][j] == y )\n",
    "\n",
    "    #m.addConstr(y == (896.7 - avg_y) / stdev_y)\n",
    "    m.setObjective(y, GRB.MAXIMIZE)\n",
    "    m.update()\n",
    "    m.setParam(\"OutputFlag\", 1)\n",
    "    m.setParam(\"MIPGap\", 1e-4)\n",
    "    m.setParam(\"NonConvex\", 2);\n",
    "    m.optimize()\n",
    "\n",
    "    #print(x)\n",
    "    #print('Cortados')\n",
    "    #print(s)\n",
    "    print('Output')\n",
    "    print(y.x)\n",
    "    print(y.x * 0.78396923 + 0.84071631)\n",
    "    print('Input')\n",
    "\n",
    "    #print(x[0,0].x)\n",
    "    #print(x[0,1].x)\n",
    "    print(x[0,0].x * std_x[0] + mean_x[0])\n",
    "    print(x[0,1].x * std_x[1] + mean_x[1])\n",
    "\n",
    "    print('binarias')\n",
    "    print(len(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output\n",
      "1.489664724989543\n",
      "2.0085676174082137\n",
      "Input\n",
      "0.8109743809687833\n",
      "-0.07623022841082316\n",
      "binarias\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "W = pickle.load(open('model.pickle', 'rb'))\n",
    "parameters = neural_init()\n",
    "model(parameters, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pickle', 'rb') as inputfile:\n",
    "    W = pickle.load(inputfile)\n",
    "\n",
    "print(W[0])\n",
    "print(len(W[0]))\n",
    "for w in W:\n",
    "    print('Weights')\n",
    "    print(w[0])\n",
    "    print('Bias')\n",
    "    print(w[1])\n",
    "    print('________________________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full model <a name=\"Fullmodel\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_model_with_bounds(parameters, weights, bounds):\n",
    "    \n",
    "    # Create the model\n",
    "    m = Model('full')\n",
    "    m.Params.LogToConsole = 0 # turn off logs\n",
    "    \n",
    "    # Local Variables\n",
    "    K = len(weights) # Number of layers in the neural network\n",
    "    nK = max(map(lambda l: len(l[0][0]), W))  # Size of the biggest layer\n",
    "\n",
    "    # Model Variables\n",
    "    x = [[m.addVar(lb=neuron[0], ub=neuron[1]) for neuron in layer] for layer in bounds[:K]] # decision variables\n",
    "    m.update()\n",
    "    for layer in x:\n",
    "        for neuron in layer:\n",
    "            if neuron.ub < 0:\n",
    "                neuron.ub = 0\n",
    "            if neuron.lb > 0:\n",
    "                neuron.lb = 0\n",
    "    m.update()\n",
    "    s = m.addVars(K, nK, lb=0, ub=10) # negative dump\n",
    "    z = m.addVars(K, nK, vtype=GRB.BINARY) # binary activation variable\n",
    "    y = m.addVar(lb=bounds[K][0][0],ub=bounds[K][0][1]) # output variable\n",
    "\n",
    "    # Input parameters\n",
    "    mean_x = parameters['mean_x']\n",
    "    std_x = parameters['std_x']\n",
    "    mean_y = parameters['mean_y']\n",
    "    std_y = parameters['std_y']\n",
    "    \n",
    "    # Constraints of the upper and lower bounds of normalized x0\n",
    "    m.addConstr( x[0][0] >= (-1 - mean_x[0])/std_x[0] ) # \"-1\" and \"1\" are the maximum and minimum values of x0, respectively\n",
    "    m.addConstr( x[0][0] <= (1 - mean_x[0])/std_x[0] )   \n",
    "    \n",
    "    # Constraints of upper and lower bounds of normalized x1\n",
    "    m.addConstr( x[0][1] >= (-1 - mean_x[1])/std_x[1] )\n",
    "    m.addConstr( x[0][1] <= (1 - mean_x[1])/std_x[1] )\n",
    "\n",
    "    for k in range(1, K+1): # Iterate over the hidden layers unitl it reaches the output layer of the neural net\n",
    "        neuronios = neurons_at(weights, k-1) # Number of neurons of each layer\n",
    "        for j in range(neuronios): # Iterate over the neurons of the layer\n",
    "            if k != K: # HIDDEN LAYERS\n",
    "                \"\"\"\n",
    "                Weights : W[layer][weights = 0 or bias = 1][inputs of the previous layer][neuron]\n",
    "                Input   : x[layer][variable]\n",
    "                Negative part : s[layer][variable]\n",
    "                \n",
    "                \"\"\" \n",
    "                \n",
    "                # Rule of the hidden layers output ( W^{k-1} * x^{k-1} + BIAS = x^k - s^k ) \n",
    "                m.addConstr( sum( weights[k-1][0][i][j] * x[k-1][i] for i in range(len(weights[k-1][0])))\n",
    "                    + weights[k-1][1][j] == x[k][j] - s[k,j] )\n",
    "                \n",
    "                \n",
    "                if bounds[k][j][1] < 0:\n",
    "                    z[k,j].lb , z[k,j].ub = 0, 0\n",
    "                if bounds[k][j][0] > 0:\n",
    "                    z[k,j].lb , z[k,j].ub = 1, 1\n",
    "                    \n",
    "                assert bounds[k][j][1] >= bounds[k][j][0]\n",
    "                \n",
    "                # Classic ReLu Formulation\n",
    "                m.addConstr( x[k][j] <= bounds[k][j][1] * z[k,j] )\n",
    "                m.addConstr( s[k,j] <= - bounds[k][j][0] * (1 - z[k,j]) )\n",
    "                m.addConstr( x[k][j] >= 0)\n",
    "                               \n",
    "            else: # OUTPUT LAYER\n",
    "                # Rule of the output layer ( W^{k-1} * x^{k-1} + BIAS = y )\n",
    "                m.addConstr( sum( weights[k-1][0][i][j] * x[k-1][i] for i in range(len(weights[k-1][0]))) + weights[k-1][1][j] == y )\n",
    "            \n",
    "    \n",
    "    m.setObjective(y, GRB.MAXIMIZE)\n",
    "    m.update()\n",
    "    m.setParam(\"OutputFlag\", 1)\n",
    "    m.setParam(\"MIPGap\", 1e-4)\n",
    "    m.setParam(\"NonConvex\", 2);\n",
    "    m.optimize()\n",
    "\n",
    "    print('Output')\n",
    "    print('Normalized {}    Real {}'.format(y.x, y.x * std_y + mean_y))\n",
    "  \n",
    "    print('Input')\n",
    "    print('Xo Normalized {}    Real {}'.format(x[0][0].x, x[0][0].x * std_x[0] + mean_x[0]))\n",
    "    print('X1 Normalized {}    Real {}'.format(x[0][1].x, x[0][1].x * std_x[1] + mean_x[1]))\n",
    "\n",
    "\n",
    "    print('binarias')\n",
    "    print(len(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAD b_RR :  1.5652317620208744 MAD b_1 :  5.209242953684459\n",
      "Output\n",
      "Normalized 1.489664724989543    Real 2.008567620056989\n",
      "Input\n",
      "Xo Normalized 1.401141587970361    Real 0.8109743809687833\n",
      "X1 Normalized -0.13170495368704388    Real -0.07623022841082316\n",
      "binarias\n",
      "40\n",
      "sucess b_RR\n",
      "Output\n",
      "Normalized 1.4896647249895425    Real 2.008567620056989\n",
      "Input\n",
      "Xo Normalized 1.401141587970361    Real 0.8109743809687833\n",
      "X1 Normalized -0.13170495368704388    Real -0.07623022841082316\n",
      "binarias\n",
      "40\n",
      "sucess b_1\n"
     ]
    }
   ],
   "source": [
    "W = pickle.load(open('model.pickle', 'rb'))\n",
    "parameters = neural_init()\n",
    "b_RR = RR(parameters, W, init_bounds(W,10))\n",
    "b_1 = prototype_1(parameters, W)\n",
    "\n",
    "print('MAD b_RR : ', MAD(b_RR),'MAD b_1 : ', MAD(b_1)) # b_1 bounds are weaker\n",
    "try:\n",
    "    main_model_with_bounds(parameters, W, b_RR)\n",
    "    print('sucess b_RR')\n",
    "except:\n",
    "    print('failure b_RR')\n",
    "    \n",
    "try:\n",
    "    main_model_with_bounds(parameters, W, b_1)\n",
    "    print('sucess b_1')\n",
    "except:\n",
    "    print('failure b_1')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Model Hyperbolic <a name=\"FullModelHyperbolic\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_model_with_bounds_flying(parameters, weights, bounds):\n",
    "    \n",
    "    # Create the model\n",
    "    m = Model('main_hiperbolic')\n",
    "    #m.Params.LogToConsole = 0 # turn off logs\n",
    "    \n",
    "    # Local Variables\n",
    "    K = len(weights) # Number of layers in the neural network\n",
    "    nK = max(map(lambda l: len(l[0][0]), W))  # Size of the biggest layer\n",
    "        \n",
    "    # Model Variables\n",
    "    x = [[m.addVar(lb=neuron[0], ub=neuron[1]) for neuron in layer] for layer in bounds[:K]] # decision variables\n",
    "    s = m.addVars(K, nK, lb=-5, ub=5) # negative dump\n",
    "    z = m.addVars(K, nK, vtype=GRB.BINARY) # binary activation variable\n",
    "    y = m.addVar(lb=bounds[K][0][0],ub=bounds[K][0][1]) # output variable\n",
    "    a = m.addVars(K, nK) # auxiliary variable\n",
    "     \n",
    "    m.update()\n",
    "    for k,layer in enumerate(x):\n",
    "        for j,neuron in enumerate(layer):\n",
    "            if neuron.ub < 0:         \n",
    "                neuron.ub = 0\n",
    "                #s[k,j].lb = - neuron.ub\n",
    "                z[k,j].lb , z[k,j].ub = 0, 0\n",
    "            if neuron.lb >= 0:            \n",
    "                #s[k,j].ub = 0\n",
    "                z[k,j].lb , z[k,j].ub = 1, 1\n",
    "            else:\n",
    "                pass\n",
    "                #s[k,j].ub = - neuron.lb\n",
    "                \n",
    "    m.update()\n",
    "    \n",
    "    \n",
    "    # Input parameters\n",
    "    mean_x = parameters['mean_x']\n",
    "    std_x = parameters['std_x']\n",
    "    mean_y = parameters['mean_y']\n",
    "    std_y = parameters['std_y']\n",
    "    \n",
    "    # Constraints of the upper and lower bounds of normalized x0\n",
    "    m.addConstr( x[0][0] >= (-1 - mean_x[0])/std_x[0] ) # \"-1\" and \"1\" are the maximum and minimum values of x0, respectively\n",
    "    m.addConstr( x[0][0] <= (1 - mean_x[0])/std_x[0] )   \n",
    "    \n",
    "    # Constraints of upper and lower bounds of normalized x1\n",
    "    m.addConstr( x[0][1] >= (-1 - mean_x[1])/std_x[1] )\n",
    "    m.addConstr( x[0][1] <= (1 - mean_x[1])/std_x[1] )\n",
    "    \n",
    "    t = 0.5\n",
    "    \n",
    "    \n",
    "    for k in range(1, K+1): # Iterate over the hidden layers unitl it reaches the output layer of the neural net\n",
    "        neuronios = neurons_at(weights, k-1) # Number of neurons of each layer\n",
    "        for j in range(neuronios): # Iterate over the neurons of the layer\n",
    "            if k != K: # HIDDEN LAYERS\n",
    "                \"\"\"\n",
    "                Weights : W[layer][weights = 0 or bias = 1][inputs of the previous layer][neuron]\n",
    "                Input   : x[layer][variable]\n",
    "                Negative part : s[layer][variable]\n",
    "                \"\"\" \n",
    "                \n",
    "                # Rule of the hidden layers output ( W^{k-1} * x^{k-1} + BIAS = x^k - s^k ) \n",
    "                m.addConstr( sum( weights[k-1][0][i][j] * x[k-1][i] for i in range(len(weights[k-1][0])))\n",
    "                    + weights[k-1][1][j] == s[k,j] )\n",
    "                \n",
    "                \n",
    "                \n",
    "                    \n",
    "                \n",
    "                m.addConstr( x[k][j] == 0.5 * (s[k,j] + a[k,j]) ) \n",
    "                m.addConstr( a[k,j] * a[k,j] == s[k,j] * s[k,j] + t * t )\n",
    "                \n",
    "                # Classic ReLu Formulation\n",
    "                m.addConstr( x[k][j] <= bounds[k][j][1] * z[k,j] )\n",
    "                m.addConstr( x[k][j] >= -bounds[k][j][0] * (1 - z[k,j]) )\n",
    "                #m.addConstr( x[k][j] >= 0)\n",
    "                               \n",
    "            else: # OUTPUT LAYER\n",
    "                # Rule of the output layer ( W^{k-1} * x^{k-1} + BIAS = y )\n",
    "                m.addConstr( sum( weights[k-1][0][i][j] * x[k-1][i] for i in range(len(weights[k-1][0]))) + weights[k-1][1][j] == y )\n",
    "            \n",
    "            \n",
    "    \n",
    "    m.setObjective(y, GRB.MAXIMIZE)\n",
    "    m.update()\n",
    "    m.setParam(\"OutputFlag\", 1)\n",
    "    m.setParam(\"MIPGap\", 1e-4)\n",
    "    m.setParam(\"NonConvex\", 2);\n",
    "    m.optimize()\n",
    "\n",
    "    print('Output')\n",
    "    print('Normalized {}    Real {}'.format(y.x, y.x * std_y + mean_y))\n",
    "  \n",
    "    print('Input')\n",
    "    print('Xo Normalized {}    Real {}'.format(x[0][0].x, x[0][0].x * std_x[0] + mean_x[0]))\n",
    "    print('X1 Normalized {}    Real {}'.format(x[0][1].x, x[0][1].x * std_x[1] + mean_x[1]))\n",
    "\n",
    "\n",
    "    print('binarias')\n",
    "    print(len(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAD b_RR :  1.5652317620208744 MAD b_0 :  5.209242953684459\n",
      "Parameter OutputFlag unchanged\n",
      "   Value: 1  Min: 0  Max: 1  Default: 1\n",
      "Parameter MIPGap unchanged\n",
      "   Value: 0.0001  Min: 0.0  Max: inf  Default: 0.0001\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "Gurobi Optimizer version 9.0.3 build v9.0.3rc0 (win64)\n",
      "Optimize a model with 125 rows, 153 columns and 475 nonzeros\n",
      "Model fingerprint: 0x97b0d499\n",
      "Model has 30 quadratic constraints\n",
      "Variable types: 113 continuous, 40 integer (40 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [7e-04, 3e+00]\n",
      "  QMatrix range    [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [5e-02, 5e+00]\n",
      "  RHS range        [2e-02, 3e+00]\n",
      "  QRHS range       [3e-01, 3e-01]\n",
      "Presolve removed 35 rows and 60 columns\n",
      "Presolve time: 0.00s\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.01 seconds\n",
      "Thread count was 1 (of 16 available processors)\n",
      "\n",
      "Solution count 0\n",
      "No other solutions better than -1e+100\n",
      "\n",
      "Model is infeasible\n",
      "Best objective -, best bound -, gap -\n",
      "Output\n",
      "failure b_RR\n",
      "Parameter OutputFlag unchanged\n",
      "   Value: 1  Min: 0  Max: 1  Default: 1\n",
      "Parameter MIPGap unchanged\n",
      "   Value: 0.0001  Min: 0.0  Max: inf  Default: 0.0001\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "Gurobi Optimizer version 9.0.3 build v9.0.3rc0 (win64)\n",
      "Optimize a model with 125 rows, 153 columns and 475 nonzeros\n",
      "Model fingerprint: 0x00b61b8f\n",
      "Model has 30 quadratic constraints\n",
      "Variable types: 113 continuous, 40 integer (40 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [7e-04, 5e+00]\n",
      "  QMatrix range    [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e-01, 5e+00]\n",
      "  RHS range        [2e-02, 5e+00]\n",
      "  QRHS range       [3e-01, 3e-01]\n",
      "Presolve removed 34 rows and 60 columns\n",
      "Presolve time: 0.00s\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.01 seconds\n",
      "Thread count was 1 (of 16 available processors)\n",
      "\n",
      "Solution count 0\n",
      "No other solutions better than -1e+100\n",
      "\n",
      "Model is infeasible\n",
      "Best objective -, best bound -, gap -\n",
      "Output\n",
      "failure b_0\n"
     ]
    }
   ],
   "source": [
    "W = pickle.load(open('model.pickle', 'rb'))\n",
    "parameters = neural_init()\n",
    "b_RR = RR(parameters, W, init_bounds(W,10))\n",
    "b_0 = prototype_1(parameters, W)\n",
    "\n",
    "print('MAD b_RR : ', MAD(b_RR),'MAD b_0 : ', MAD(b_0)) # b_0 bounds are weaker\n",
    "try:\n",
    "    main_model_with_bounds_flying(parameters, W, b_RR)\n",
    "    print('sucess b_RR')\n",
    "except:\n",
    "    print('failure b_RR')\n",
    "    \n",
    "try:\n",
    "    main_model_with_bounds_flying(parameters, W, b_0)\n",
    "    print('sucess b_0')\n",
    "except:\n",
    "    print('failure b_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter OutputFlag unchanged\n",
      "   Value: 1  Min: 0  Max: 1  Default: 1\n",
      "Parameter MIPGap unchanged\n",
      "   Value: 0.0001  Min: 0.0  Max: inf  Default: 0.0001\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "Gurobi Optimizer version 9.0.3 build v9.0.3rc0 (win64)\n",
      "Optimize a model with 125 rows, 153 columns and 475 nonzeros\n",
      "Model fingerprint: 0x97b0d499\n",
      "Model has 30 quadratic constraints\n",
      "Variable types: 113 continuous, 40 integer (40 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [7e-04, 3e+00]\n",
      "  QMatrix range    [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [5e-02, 5e+00]\n",
      "  RHS range        [2e-02, 3e+00]\n",
      "  QRHS range       [3e-01, 3e-01]\n",
      "Presolve removed 35 rows and 60 columns\n",
      "Presolve time: 0.00s\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.01 seconds\n",
      "Thread count was 1 (of 16 available processors)\n",
      "\n",
      "Solution count 0\n",
      "No other solutions better than -1e+100\n",
      "\n",
      "Model is infeasible\n",
      "Best objective -, best bound -, gap -\n",
      "Output\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Unable to retrieve attribute 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-afe7d6e8f81f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain_model_with_bounds_flying\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_RR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-115-b825a017b5b5>\u001b[0m in \u001b[0;36mmain_model_with_bounds_flying\u001b[1;34m(parameters, weights, bounds)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Normalized {}    Real {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mstd_y\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmean_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Input'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mvar.pxi\u001b[0m in \u001b[0;36mgurobipy.Var.__getattr__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mvar.pxi\u001b[0m in \u001b[0;36mgurobipy.Var.getAttr\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mattrutil.pxi\u001b[0m in \u001b[0;36mgurobipy.__getattr\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Unable to retrieve attribute 'x'"
     ]
    }
   ],
   "source": [
    "main_model_with_bounds_flying(parameters, W, b_RR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net <a name=\"NeuralNet\"></a>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "#  Cria um array de -1 a 1 espaçado 400 vezes iguais\n",
    "x1 = np.linspace(-1, 1, 400) # var 1\n",
    "x2 = np.linspace(-1, 1, 400) # var 2\n",
    "\n",
    "x = np.array(list(itertools.product(x1, x2)))\n",
    "\n",
    "# Função descrita pela rede neural\n",
    "y = np.sin(2 * x[:, 0]) + np.cos(x[:, 1])\n",
    "\n",
    "#  Média e Desvio Padrão da entrada\n",
    "mean_x = np.mean(x, axis=0)\n",
    "std_x = np.std(x, axis=0)\n",
    "\n",
    "#  Média e Desvio Padrão da saída\n",
    "mean_y = np.mean(y, axis=0)\n",
    "std_y = np.std(y, axis=0)\n",
    "\n",
    "parameters = {'x': x, 'y': y,\n",
    "            'mean_x': mean_x, 'mean_y': mean_y,\n",
    "            'std_x': std_x, 'std_y': std_y}\n",
    "\n",
    "x = parameters['x']\n",
    "y = parameters['y']\n",
    "\n",
    "mean_x = parameters[\"mean_x\"]\n",
    "print(\"mean_x =\", mean_x)\n",
    "\n",
    "std_x = parameters['std_x']\n",
    "print(\"std_x =\", std_x)\n",
    "\n",
    "mean_y = parameters['mean_y']\n",
    "print(\"mean_y =\", mean_y)\n",
    "\n",
    "std_y = parameters['std_y']\n",
    "print(\"std_y =\", std_y)\n",
    "\n",
    "#  Normalização da entrada e da saída\n",
    "x_norm = (x - mean_x) / std_x\n",
    "y_norm = (y - mean_y) / std_y\n",
    "\n",
    "#  Camadas da Rede Neural ReLu\n",
    "model = keras.models.Sequential()\n",
    "model.add(Dense(10, input_dim=2, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_norm, y_norm, epochs=20)  # nb_epoch=20\n",
    "\n",
    "res = model.evaluate(x_norm, y_norm)\n",
    "\n",
    "print(res)\n",
    "\n",
    "print(\"mean_x =\", mean_x)\n",
    "print(\"std_x =\", std_x)\n",
    "\n",
    "print(\"mean_y =\", mean_y)\n",
    "print(\"std_y =\", std_y)\n",
    "\n",
    "weights = list(map(lambda layer: layer.get_weights(), model.layers))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous Implementations <a name=\"PreviousImplementations\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicialize_bounds(weights):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    weights : Weights of the neural network\n",
    "    \n",
    "    Output\n",
    "    bounds : a zeros numpy array shapped as (total_neurons + len(input_dim), input_dim)\n",
    "    \"\"\"\n",
    "    t = total_neurons(weights) + len(weights[0])\n",
    "    bounds = np.zeros((t,2))\n",
    "    return bounds\n",
    "\n",
    "\n",
    "def bound_model(model, x, s, k, j, maximize):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    model : model to optimize\n",
    "    x, s  : variables of the model\n",
    "    k, j  : layer and neuron, respectively\n",
    "    maximize : objetive of the model (True == maximize False == minimize)\n",
    "    \n",
    "    Output\n",
    "    new_bound : bound generated after the bound tightening procedure\n",
    "    \"\"\"\n",
    "    if maximize:\n",
    "        model.setObjective(x[k,j] - s[k,j], GRB.MAXIMIZE)\n",
    "        model.optimize()\n",
    "        new_bound = model.ObjVal\n",
    "    else:\n",
    "        model.setObjective(x[k,j] - s[k,j], GRB.MINIMIZE)\n",
    "        model.optimize()\n",
    "        new_bound = model.ObjVal\n",
    "    return new_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bound_tightening(parameters, weights):\n",
    "    \n",
    "    # Create the model\n",
    "    m = Model('bound_tightening')\n",
    "    m.Params.LogToConsole = 0 # turn off logs\n",
    "    \n",
    "    # Local Variables\n",
    "    bounds = inicialize_bounds(weights)\n",
    "    aux = 2 # index of the bounds array\n",
    "    K = len(weights) # Number of layers in the neural network\n",
    "    nK = max(map(lambda l: len(l[0][0]), W))  # Size of the biggest layer\n",
    "    \n",
    "    # Model Variables\n",
    "    x = m.addVars(K, nK,lb=-5,ub=5) # decision variables\n",
    "    s = m.addVars(K, nK, lb=0, ub=10) # negative dump\n",
    "    z = m.addVars(K, nK, lb=0, ub=1) # binary activation variable\n",
    "    y = m.addVar(lb=-5,ub=5) # output variable\n",
    "       \n",
    "    # Input parameters\n",
    "    mean_x = parameters['mean_x']\n",
    "    std_x = parameters['std_x']\n",
    "    mean_y = parameters['mean_y']\n",
    "    std_y = parameters['std_y']\n",
    "    \n",
    "    # Constraints of the upper and lower bounds of normalized x0\n",
    "    m.addConstr( x[0,0] >= (-1 - mean_x[0])/std_x[0] ) # \"-1\" and \"1\" are the maximum and minimum values of x0, respectively\n",
    "    m.addConstr( x[0,0] <= (1 - mean_x[0])/std_x[0] )   \n",
    "    \n",
    "    # Constraints of upper and lower bounds of normalized x1\n",
    "    m.addConstr( x[0,1] >= (-1 - mean_x[1])/std_x[1] )\n",
    "    m.addConstr( x[0,1] <= (1 - mean_x[1])/std_x[1] )\n",
    "    \n",
    "    \n",
    "    for i in range(len(W[0])): # iterates over the neural net inputs\n",
    "        # Bounds of input\n",
    "        m.setObjective(x[0,i], GRB.MAXIMIZE)\n",
    "        m.optimize()\n",
    "        ub = m.ObjVal\n",
    "        m.setObjective(x[0,i], GRB.MINIMIZE)\n",
    "        m.optimize()\n",
    "        lb = m.ObjVal\n",
    "        bounds[i,1], bounds[i,0] = lb, ub\n",
    "    \n",
    "    for k in range(1, K+1): # Iterate over the hidden layers unitl it reaches the output layer of the neural net\n",
    "        neuronios = neurons_at(weights, k-1) # Number of neurons of each layer\n",
    "        for j in range(neuronios): # Iterate over the neurons of the layer\n",
    "            if k != K: # HIDDEN LAYERS\n",
    "                \"\"\"\n",
    "                Weights : W[layer][weights = 0 or bias = 1][inputs of the previous layer][neuron]\n",
    "                Input   : x[layer][variable]\n",
    "                Negative part : s[layer][variable]\n",
    "                \n",
    "                \"\"\" \n",
    "                \n",
    "                # Rule of the hidden layers output ( W^{k-1} * x^{k-1} + BIAS = x^k - s^k ) \n",
    "                m.addConstr( sum( weights[k-1][0][i][j] * x[k-1,i] for i in range(len(weights[k-1][0])))\n",
    "                    + weights[k-1][1][j] == x[k,j] - s[k,j] )\n",
    "                \n",
    "                \n",
    "                bounds[aux,0] = bound_model(m, x, s, k, j, True)\n",
    "                bounds[aux,1] = bound_model(m, x, s, k, j, False)\n",
    "                \n",
    "                # Classic ReLu Formulation\n",
    "                m.addConstr( x[k,j] <= bounds[aux,0] * z[k,j] )\n",
    "                m.addConstr( s[k,j] <= -bounds[aux,1] * (1 - z[k,j]) )\n",
    "                m.addConstr( x[k,j] >= 0)\n",
    "                               \n",
    "            else: # OUTPUT LAYER\n",
    "                # Rule of the output layer ( W^{k-1} * x^{k-1} + BIAS = y )\n",
    "                m.addConstr( sum( weights[k-1][0][i][j] * x[k-1,i] for i in range(len(weights[k-1][0]))) + weights[k-1][1][j] == y )\n",
    "                \n",
    "                # Bounds of output\n",
    "                m.setObjective(y, GRB.MAXIMIZE)\n",
    "                m.optimize()\n",
    "                ub = m.ObjVal\n",
    "                m.setObjective(y, GRB.MINIMIZE)\n",
    "                m.optimize()\n",
    "                lb = m.ObjVal\n",
    "                bounds[aux,1], bounds[aux,0] = lb, ub\n",
    "                print(lb,ub)\n",
    "            \n",
    "            aux += 1\n",
    "    \n",
    "    return bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.246551803949021 1.7364051413861032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.72772608, -1.72772608],\n",
       "       [ 1.72772608, -1.72772608],\n",
       "       [ 0.64205439, -0.27340875],\n",
       "       [ 0.96719348, -0.73286807],\n",
       "       [ 1.17378792, -0.48834118],\n",
       "       [ 0.2331374 , -1.15255043],\n",
       "       [ 1.15094301, -0.25402936],\n",
       "       [ 1.03169151, -0.20633246],\n",
       "       [ 0.80162674, -0.75380247],\n",
       "       [ 0.50661042, -0.83868536],\n",
       "       [ 1.05711069, -0.11309143],\n",
       "       [ 1.04564983, -0.71581283],\n",
       "       [-0.22943133, -1.18742514],\n",
       "       [ 0.82950059, -0.24681179],\n",
       "       [ 0.5584421 , -0.26065242],\n",
       "       [ 0.89848343, -0.49654192],\n",
       "       [ 0.96073047, -0.16462877],\n",
       "       [ 1.21993548, -0.34380913],\n",
       "       [ 1.12247633, -0.07704481],\n",
       "       [ 0.27382051, -0.70819428],\n",
       "       [ 0.75028852,  0.05015983],\n",
       "       [ 0.72975842, -0.18671328],\n",
       "       [ 0.60837369, -0.68422108],\n",
       "       [ 1.00806358, -0.44320637],\n",
       "       [ 1.5012147 , -0.13275513],\n",
       "       [ 1.15488415, -0.18326089],\n",
       "       [ 1.03560569, -0.67558208],\n",
       "       [ 0.2016631 , -2.89911724],\n",
       "       [ 1.20067041, -0.56544154],\n",
       "       [ 0.3239155 , -0.88512262],\n",
       "       [-0.15487346, -0.9954302 ],\n",
       "       [ 1.40181555, -0.13892282],\n",
       "       [ 1.73640514, -2.2465518 ]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = pickle.load(open('model.pickle', 'rb'))\n",
    "parameters = neural_init()\n",
    "bounds = bound_tightening(parameters, W)\n",
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_model_with_bounds(parameters, weights, bounds):\n",
    "    \n",
    "    # Create the model\n",
    "    m = Model('bt')\n",
    "    m.Params.LogToConsole = 0 # turn off logs\n",
    "    \n",
    "    # Local Variables\n",
    "    aux = 2 # index of the bounds array\n",
    "    K = len(weights) # Number of layers in the neural network\n",
    "    nK = max(map(lambda l: len(l[0][0]), W))  # Size of the biggest layer\n",
    "    \n",
    "    # Model Variables\n",
    "    x = m.addVars(K, nK,lb=bounds[0,1],ub=bounds[0,0]) # decision variables\n",
    "    s = m.addVars(K, nK, lb=0, ub=10) # negative dump\n",
    "    z = m.addVars(K, nK, vtype=GRB.BINARY) # binary activation variable\n",
    "    y = m.addVar(lb=bounds[-1,1],ub=bounds[-1,0]) # output variable\n",
    "\n",
    "    # Input parameters\n",
    "    mean_x = parameters['mean_x']\n",
    "    std_x = parameters['std_x']\n",
    "    mean_y = parameters['mean_y']\n",
    "    std_y = parameters['std_y']\n",
    "    \n",
    "    # Constraints of the upper and lower bounds of normalized x0\n",
    "    m.addConstr( x[0,0] >= (-1 - mean_x[0])/std_x[0] ) # \"-1\" and \"1\" are the maximum and minimum values of x0, respectively\n",
    "    m.addConstr( x[0,0] <= (1 - mean_x[0])/std_x[0] )   \n",
    "    \n",
    "    # Constraints of upper and lower bounds of normalized x1\n",
    "    m.addConstr( x[0,1] >= (-1 - mean_x[1])/std_x[1] )\n",
    "    m.addConstr( x[0,1] <= (1 - mean_x[1])/std_x[1] )\n",
    "\n",
    "    for k in range(1, K+1): # Iterate over the hidden layers unitl it reaches the output layer of the neural net\n",
    "        neuronios = neurons_at(weights, k-1) # Number of neurons of each layer\n",
    "        for j in range(neuronios): # Iterate over the neurons of the layer\n",
    "            if k != K: # HIDDEN LAYERS\n",
    "                \"\"\"\n",
    "                Weights : W[layer][weights = 0 or bias = 1][inputs of the previous layer][neuron]\n",
    "                Input   : x[layer][variable]\n",
    "                Negative part : s[layer][variable]\n",
    "                \n",
    "                \"\"\" \n",
    "                \n",
    "                # Rule of the hidden layers output ( W^{k-1} * x^{k-1} + BIAS = x^k - s^k ) \n",
    "                m.addConstr( sum( weights[k-1][0][i][j] * x[k-1,i] for i in range(len(weights[k-1][0])))\n",
    "                    + weights[k-1][1][j] == x[k,j] - s[k,j] )\n",
    "                \n",
    "                # Classic ReLu Formulation\n",
    "                if bounds[aux,0] < 0:\n",
    "                    z[k,j].lb , z[k,j].ub = 0, 0\n",
    "                if bounds[aux,1] > 0:\n",
    "                    z[k,j].lb , z[k,j].ub = 1, 1\n",
    "                    \n",
    "                m.addConstr( x[k,j] <= bounds[aux,0] * z[k,j] )\n",
    "                m.addConstr( s[k,j] <= - bounds[aux,1] * (1 - z[k,j]) )\n",
    "                m.addConstr( x[k,j] >= 0)\n",
    "                               \n",
    "            else: # OUTPUT LAYER\n",
    "                # Rule of the output layer ( W^{k-1} * x^{k-1} + BIAS = y )\n",
    "                m.addConstr( sum( weights[k-1][0][i][j] * x[k-1,i] for i in range(len(weights[k-1][0]))) + weights[k-1][1][j] == y )\n",
    "            \n",
    "            aux += 1\n",
    "    \n",
    "    m.setObjective(y, GRB.MAXIMIZE)\n",
    "    m.update()\n",
    "    m.setParam(\"OutputFlag\", 1)\n",
    "    m.setParam(\"MIPGap\", 1e-4)\n",
    "    m.setParam(\"NonConvex\", 2);\n",
    "    m.optimize()\n",
    "\n",
    "    print('Output')\n",
    "    print('Normalized {}    Real {}'.format(y.x, y.x * std_y + mean_y))\n",
    "  \n",
    "    print('Input')\n",
    "    print('Xo Normalized {}    Real {}'.format(x[0,0].x, x[0,0].x * std_x[0] + mean_x[0]))\n",
    "    print('X1 Normalized {}    Real {}'.format(x[0,1].x, x[0,1].x * std_x[1] + mean_x[1]))\n",
    "\n",
    "\n",
    "    print('binarias')\n",
    "    print(len(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bound_tightening' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-604f48c38e2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneural_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbound_tightening\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmain_model_with_bounds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bound_tightening' is not defined"
     ]
    }
   ],
   "source": [
    "W = pickle.load(open('model.pickle', 'rb'))\n",
    "parameters = neural_init()\n",
    "b = bound_tightening(parameters, W)\n",
    "main_model_with_bounds(parameters, W, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
